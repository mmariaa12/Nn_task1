{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "problem_solved.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eeb7532df2be4cf2ba25691eabeb48d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb00df8ffc1a4eae933e70287ed9bf48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_582a0b0515dc4e8096abffaf51506f6a",
              "IPY_MODEL_fd5e7d44dd5d4782a6f9f5ee066ec923"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "fb00df8ffc1a4eae933e70287ed9bf48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "582a0b0515dc4e8096abffaf51506f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81b4998273d5484ea8362089c2f0ae70",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 45,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 45,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_627297e51f824f90911882ff54500c89"
          },
          "model_module_version": "1.5.0"
        },
        "fd5e7d44dd5d4782a6f9f5ee066ec923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3167c62a5cac4fcab95e81d1fe5a8a6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 45/45 [03:28&lt;00:00,  4.63s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ae0b54c54294174b8428a2fd1f25a90"
          },
          "model_module_version": "1.5.0"
        },
        "81b4998273d5484ea8362089c2f0ae70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "627297e51f824f90911882ff54500c89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "3167c62a5cac4fcab95e81d1fe5a8a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "9ae0b54c54294174b8428a2fd1f25a90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "b89ce09305dd41a18deb794ce9e7abbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba564b750d86485e82901e28137d4fa0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ebaf8c08b4104513809b5dfefd3f6517",
              "IPY_MODEL_7ef64ea579804abf9976e0a2100e14a3"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "ba564b750d86485e82901e28137d4fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "ebaf8c08b4104513809b5dfefd3f6517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_83ccdf2320154bf2a51aba0ea5be1940",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 45,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 45,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30788e32776a478e9690bc34e6b12280"
          },
          "model_module_version": "1.5.0"
        },
        "7ef64ea579804abf9976e0a2100e14a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4cc07e25ecac4f9fbfa825d35e1b5bb0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 45/45 [03:14&lt;00:00,  4.32s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_769399236a864646b3b8b9b278622eae"
          },
          "model_module_version": "1.5.0"
        },
        "83ccdf2320154bf2a51aba0ea5be1940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "30788e32776a478e9690bc34e6b12280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "4cc07e25ecac4f9fbfa825d35e1b5bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "769399236a864646b3b8b9b278622eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "ffa971f148bc42ecb4e173f94b8a1739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_023b5d9fd72846af839cf2bcc5c9847d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4edc65a77c5741728dcbb9c9589fb03a",
              "IPY_MODEL_436d9940170d42fcb3ff16db58e1e9f4"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "023b5d9fd72846af839cf2bcc5c9847d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "4edc65a77c5741728dcbb9c9589fb03a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d6c23b5176ac41a48d3131eb5c536e08",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 45,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 45,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f048431f8081478094558c41b101ed7f"
          },
          "model_module_version": "1.5.0"
        },
        "436d9940170d42fcb3ff16db58e1e9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_487a811daf9544bea5886f00c784c5ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 45/45 [03:16&lt;00:00,  4.36s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89e4753bd37c430f87394deb0942bcd5"
          },
          "model_module_version": "1.5.0"
        },
        "d6c23b5176ac41a48d3131eb5c536e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "f048431f8081478094558c41b101ed7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "487a811daf9544bea5886f00c784c5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "89e4753bd37c430f87394deb0942bcd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "31052ab1ca834bda8b95b72ea6c17989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5b58de45e885424fb3daa6bd6d697cef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eea53d26cb534f93972921f3e2376591",
              "IPY_MODEL_22dc30bcd04347198d9e4d977bd6cb88"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "5b58de45e885424fb3daa6bd6d697cef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "eea53d26cb534f93972921f3e2376591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2df74b8b1be849de9421bc291db7ed6c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 45,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 45,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b1aece8cda44142910d80b28771469a"
          },
          "model_module_version": "1.5.0"
        },
        "22dc30bcd04347198d9e4d977bd6cb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3f6032308a54b0584b6593d288a8b24",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 45/45 [04:04&lt;00:00,  5.43s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5fd15a514e1144418ab6408e0fd9f7b1"
          },
          "model_module_version": "1.5.0"
        },
        "2df74b8b1be849de9421bc291db7ed6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "1b1aece8cda44142910d80b28771469a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "a3f6032308a54b0584b6593d288a8b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "5fd15a514e1144418ab6408e0fd9f7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atqZGIIyNSBb"
      },
      "source": [
        "#**Практическое задание №1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga5g3lUhNNBy"
      },
      "source": [
        "Установка необходимых пакетов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGBk36LpukIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46855103-4b43-4926-fda2-0b961f0bd241"
      },
      "source": [
        "!pip install -q libtiff\n",
        "!pip install -q tqdm\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 28.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 17.9 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 129 kB 7.5 MB/s \n",
            "\u001b[?25h  Building wheel for libtiff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vQDLyHEO1Ux"
      },
      "source": [
        "Монтирование Вашего Google Drive к текущему окружению:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G5KkA1Nu5M9",
        "outputId": "d90dbcdb-a9b8-4d7d-a6a7-95e88f357732"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "base_dir ='drive/MyDrive/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6-mtI6W1y1b"
      },
      "source": [
        "В переменную PROJECT_DIR необходимо прописать путь к директории на Google Drive, в которую Вы загрузили zip архивы с предоставленными наборами данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdvM-BUTvfSV"
      },
      "source": [
        "# todo\n",
        "#PROJECT_DIR='./'\n",
        "PROJECT_DIR = 'dev/prak_nn_1/'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Num5lHV6912"
      },
      "source": [
        "Константы, которые пригодятся в коде далее:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab2yCwDm7Fqb"
      },
      "source": [
        "EVALUATE_ONLY = True\n",
        "TEST_ON_LARGE_DATASET = False\n",
        "TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS_LINKS = {\n",
        "    'train': '1XtQzVQ5XbrfxpLHJuL0XBGJ5U7CS-cLi',\n",
        "    'train_small': '1qd45xXfDwdZjktLFwQb-et-mAaFeCzOR',\n",
        "    'train_tiny': '1I-2ZOuXLd4QwhZQQltp817Kn3J0Xgbui',\n",
        "    'test': '1RfPou3pFKpuHDJZ-D9XDFzgvwpUBFlDr',\n",
        "    'test_small': '1wbRsog0n7uGlHIPGLhyN-PMeT2kdQ2lI',\n",
        "    'test_tiny': '1viiB0s041CNsAK4itvX8PnYthJ-MDnQc'\n",
        "}"
      ],
      "metadata": {
        "id": "ocS1yCumHEsE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgY-ux5qOI0k"
      },
      "source": [
        "Импорт необходимых зависимостей:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install libtiff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4vzgtceHTge",
        "outputId": "7d83f8ea-c0ec-4484-d2f4-56b9774a3da0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: libtiff in /usr/local/lib/python3.7/dist-packages (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLHQhqiSIyvK"
      },
      "source": [
        "from pathlib import Path\n",
        "from libtiff import TIFF\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "from sklearn.metrics import balanced_accuracy_score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLI3lUyMYO9"
      },
      "source": [
        "---\n",
        "### Класс Dataset\n",
        "\n",
        "Предназначен для работы с наборами данных, хранящихся на Google Drive, обеспечивает чтение изображений и соответствующих меток, а также формирование пакетов (батчей)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N169efsw1ej"
      },
      "source": [
        "import gdown\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.is_loaded = False\n",
        "        if not Path(f'{name}.npz').exists():\n",
        "            url = f'https://drive.google.com/uc?id={DATASETS_LINKS[name]}'\n",
        "            output = f'{name}.npz'\n",
        "            gdown.download(url, output, quiet=False)\n",
        "        print(f'Loading dataset {self.name} from npz.')\n",
        "        np_obj = np.load(f'{name}.npz')\n",
        "        self.images = np_obj['data']\n",
        "        self.labels = np_obj['labels']\n",
        "        self.n_files = self.images.shape[0]\n",
        "        self.is_loaded = True\n",
        "        print(f'Done. Dataset {name} consists of {self.n_files} images.')\n",
        "\n",
        "    def image(self, i):\n",
        "        # read i-th image in dataset and return it as numpy array\n",
        "        if self.is_loaded:\n",
        "            return self.images[i, :, :, :]\n",
        "\n",
        "    def images_seq(self, start=0, n=None):\n",
        "        # sequential access to images inside dataset (is needed for testing)\n",
        "        for i in range(start, start + self.n_files if not n else start + n):\n",
        "            yield self.image(i)\n",
        "\n",
        "    def random_image_with_label(self):\n",
        "        # get random image with label from dataset\n",
        "        i = np.random.randint(self.n_files)\n",
        "        return self.image(i), self.labels[i]\n",
        "  \n",
        "    def random_batch_with_labels(self, n):\n",
        "        # create random batch of images with labels (is needed for training)\n",
        "        indices = np.random.choice(self.n_files, n)\n",
        "        imgs = []\n",
        "        for i in indices:\n",
        "            img = self.image(i)\n",
        "            imgs.append(self.image(i))\n",
        "        logits = np.array([self.labels[i] for i in indices])\n",
        "        return np.stack(imgs), logits\n",
        "\n",
        "    def image_with_label(self, i: int):\n",
        "        # return i-th image with label from dataset\n",
        "        return self.image(i), self.labels[i]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-LvGqeHYgus"
      },
      "source": [
        "### Пример использвания класса Dataset\n",
        "Загрузим обучающий набор данных, получим произвольное изображение с меткой. После чего визуализируем изображение, выведем метку. В будущем, этот кусок кода можно закомментировать или убрать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "HhObWEjGJ1um",
        "outputId": "45907d99-a555-4ebf-aeae-8bc5cd4efb27"
      },
      "source": [
        "d_train_tiny = Dataset('train_tiny')\n",
        "\n",
        "img, lbl = d_train_tiny.random_image_with_label()\n",
        "print()\n",
        "print(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\n",
        "print(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\n",
        "\n",
        "pil_img = Image.fromarray(img)\n",
        "IPython.display.display(pil_img)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I-2ZOuXLd4QwhZQQltp817Kn3J0Xgbui\n",
            "To: /content/train_tiny.npz\n",
            "100%|██████████| 105M/105M [00:00<00:00, 193MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset train_tiny from npz.\n",
            "Done. Dataset train_tiny consists of 900 images.\n",
            "\n",
            "Got numpy array of shape (224, 224, 3), and label with code 6.\n",
            "Label code corresponds to NORM class.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F0B69731610>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaBXXCWeVLYb"
      },
      "source": [
        "---\n",
        "### Класс Metrics\n",
        "\n",
        "Реализует метрики точности, используемые для оценивания модели:\n",
        "1. точность,\n",
        "2. сбалансированную точность."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5unQ7azTinCZ"
      },
      "source": [
        "class Metrics:\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(gt: List[int], pred: List[int]):\n",
        "        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n",
        "        print([f'{i[0]} : {i[1]}' for i in list(zip(gt, pred))[:5]])\n",
        "        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy_balanced(gt: List[int], pred: List[int]):\n",
        "        return balanced_accuracy_score(gt, pred)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_all(gt: List[int], pred: List[int], info: str):\n",
        "        print(f'metrics for {info}:')\n",
        "        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n",
        "        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1AHzTJVGU5k"
      },
      "source": [
        "---\n",
        "### Класс Model\n",
        "\n",
        "Класс, хранящий в себе всю информацию о модели.\n",
        "\n",
        "Вам необходимо реализовать методы save, load для сохранения и заргрузки модели. Особенно актуально это будет во время тестирования на дополнительных наборах данных.\n",
        "\n",
        "> *Пожалуйста, убедитесь, что сохранение и загрузка модели работает корректно. Для этого обучите модель, протестируйте, сохраните ее в файл, перезапустите среду выполнения, загрузите обученную модель из файла, вновь протестируйте ее на тестовой выборке и убедитесь в том, что получаемые метрики совпадают с полученными для тестовой выбрки ранее.*\n",
        "\n",
        "\n",
        "Также, Вы можете реализовать дополнительные функции, такие как:\n",
        "1. валидацию модели на части обучающей выборки;\n",
        "2. использование кроссвалидации;\n",
        "3. автоматическое сохранение модели при обучении;\n",
        "4. загрузку модели с какой-то конкретной итерации обучения (если используется итеративное обучение);\n",
        "5. вывод различных показателей в процессе обучения (например, значение функции потерь на каждой эпохе);\n",
        "6. построение графиков, визуализирующих процесс обучения (например, график зависимости функции потерь от номера эпохи обучения);\n",
        "7. автоматическое тестирование на тестовом наборе/наборах данных после каждой эпохи обучения (при использовании итеративного обучения);\n",
        "8. автоматический выбор гиперпараметров модели во время обучения;\n",
        "9. сохранение и визуализацию результатов тестирования;\n",
        "10. Использование аугментации и других способов синтетического расширения набора данных (дополнительным плюсом будет обоснование необходимости и обоснование выбора конкретных типов аугментации)\n",
        "11. и т.д.\n",
        "\n",
        "Полный список опций и дополнений приведен в презентации с описанием задания.\n",
        "\n",
        "При реализации дополнительных функций допускается добавление параметров в существующие методы и добавление новых методов в класс модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pkMiB6mJ7JQ",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9f87e5-efaf-435f-ef30-6876b9317e4e"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import cv2\n",
        "import os\n",
        "from random import random, randint\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "%load_ext tensorboard\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Using architecture from https://openaccess.thecvf.com/content_cvpr_2016/papers/Hou_Patch-Based_Convolutional_Neural_CVPR_2016_paper.pdf\n",
        "        \n",
        "        self.input_shape=(224, 224, 3)\n",
        "\n",
        "        self.preproc_batch = self.preproc_batch_default\n",
        "        self.model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Conv2D(filters=64, kernel_size=5, strides=1, input_shape=self.input_shape, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=3),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=128, kernel_size=5, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.MaxPool2D(pool_size=3, strides=2),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=256, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.ReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=256, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.ReLU(),\n",
        "          tf.keras.layers.MaxPool2D(pool_size=3, strides=1),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=512, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.ReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=512, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.ReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=1024, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.ReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2D(filters=1024, kernel_size=3, kernel_regularizer=l2(.0003)),\n",
        "          tf.keras.layers.ReLU(),\n",
        "\n",
        "          tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "          tf.keras.layers.Dense(units=1024, activation='relu'),\n",
        "          tf.keras.layers.Dropout(rate=0.5),\n",
        "\n",
        "          tf.keras.layers.Dense(units=1024, activation='relu'),\n",
        "          tf.keras.layers.Dropout(rate=0.5),\n",
        "\n",
        "          tf.keras.layers.Dense(units=9, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        self.save_path=base_dir + PROJECT_DIR + 'best.h5'\n",
        "        self.checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=self.save_path,\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        self.model.compile(optimizer='adam',\n",
        "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                           metrics=['accuracy'])\n",
        "        \n",
        "        self.logdir = base_dir + PROJECT_DIR + 'logs'\n",
        "        self.tensorboard_callback = tf.keras.callbacks.TensorBoard(self.logdir, histogram_freq=1)\n",
        "\n",
        "    def summary(self):\n",
        "      print(self.model.summary())\n",
        "\n",
        "    def save(self, name: str):\n",
        "        # Unnecessary, the callback saves the best model instance during training\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def load(name: str):\n",
        "        model = Model()\n",
        "        if os.path.exists(base_dir + PROJECT_DIR + f'{name}.h5'):\n",
        "          model.model = tf.keras.models.load_model(base_dir + PROJECT_DIR + f'{name}.h5')\n",
        "\n",
        "        if 'best_densenet' in name:\n",
        "          print('Changing preprocessor')\n",
        "          model.preproc_batch = model.preproc_batch_densenset\n",
        "        elif 'best_resnet' in name:\n",
        "          print('Changing preprocessor')\n",
        "          model.preproc_batch = model.preproc_batch_resnet\n",
        "        else:\n",
        "          model.preproc_batch = model.preproc_batch_default\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, train_ds):\n",
        "        print(f'training started')\n",
        "        \n",
        "        for i in range(10):\n",
        "\n",
        "          n = 500\n",
        "          x_train, y_train = train_ds.random_batch_with_labels(n)\n",
        "          x_train = self.preproc_batch(x_train)\n",
        "\n",
        "          x_val, y_val = train_ds.random_batch_with_labels(150)\n",
        "          x_val = self.preproc_batch(x_val, 0)\n",
        "\n",
        "          print(x_train.shape, y_train.shape)\n",
        "          self.model.fit(x_train, y_train, epochs=40, callbacks=[self.checkpoint_callback], validation_data=(x_val, y_val))\n",
        "\n",
        "        print(f'training done')\n",
        "        pass\n",
        "\n",
        "    def preproc_batch_default(self, batch, thr=.5):\n",
        "      imgs = []\n",
        "      for image in batch:\n",
        "        if random() < thr:\n",
        "          border = randint(10, 40)\n",
        "          image = image[border:-border, border:-border]\n",
        "        img = cv2.resize(image/255., self.input_shape[:2], interpolation=cv2.INTER_CUBIC)\n",
        "        if random() < thr:\n",
        "          img = img[:, ::-1]\n",
        "        if random() < thr:\n",
        "          img = img[::-1]\n",
        "        img -= np.mean(img)\n",
        "        img /= (np.std(img) + 0.00001)\n",
        "        imgs.append(img)\n",
        "\n",
        "      return np.stack(imgs)\n",
        "\n",
        "    def preproc_batch_densenset(self, batch, thr=None):\n",
        "      imgs = []\n",
        "      for image in batch:\n",
        "        imgs.append(tf.keras.applications.densenet.preprocess_input(image.astype(np.float)))\n",
        "\n",
        "      return np.stack(imgs)\n",
        "\n",
        "    def preproc_batch_resnet(self, batch, thr=None):\n",
        "      imgs = []\n",
        "      for image in batch:\n",
        "        imgs.append(tf.keras.applications.resnet_v2.preprocess_input(image.astype(np.float)))\n",
        "\n",
        "      return np.stack(imgs)\n",
        "\n",
        "    def preproc_batch_deprecated(self, batch, thr=.5):\n",
        "      imgs = []\n",
        "      for image in batch:\n",
        "        if random() < thr:\n",
        "          border = randint(10, 40)\n",
        "          image = image[border:-border, border:-border]\n",
        "        img = cv2.resize(image/255., self.input_shape[:2], interpolation=cv2.INTER_CUBIC)\n",
        "        if random() < thr:\n",
        "          img = img[:, ::-1]\n",
        "        if random() < thr:\n",
        "          img = img[::-1]\n",
        "        imgs.append(img)\n",
        "\n",
        "      return np.stack(imgs)\n",
        "\n",
        "    def test_on_dataset(self, dataset: Dataset, limit=None):\n",
        "        # you can upgrade this code if you want to speed up testing using batches\n",
        "        predictions = []\n",
        "        n = dataset.n_files if not limit else int(dataset.n_files * limit)\n",
        "        batch_size = 100\n",
        "        current = 0\n",
        "        for current in tqdm(range(0, n, min(batch_size, n - current))):\n",
        "          for img in self.preproc_batch(dataset.images_seq(current, min(batch_size, n - current)), 0):\n",
        "            predictions.append(self.test_on_image(img[np.newaxis, :]))\n",
        "        return predictions\n",
        "\n",
        "    def test_on_image(self, img: np.ndarray):\n",
        "      prediction = self.model.predict(img)\n",
        "      return np.argmax(prediction)\n",
        "\n",
        "    @staticmethod\n",
        "    def stacked_test(test_ds, models):\n",
        "\n",
        "      preds_clean = np.zeros((len(models), test_ds.n_files), dtype=np.int8)\n",
        "      for i, name in enumerate(models):\n",
        "        model = Model.load(name)\n",
        "        preds_clean[i] = model.test_on_dataset(test_ds, limit=1.)\n",
        "        print(f'Model {name} evaluated')\n",
        "\n",
        "      preds = preds_clean.T\n",
        "\n",
        "      preds_stacked = np.zeros(len(preds), dtype=np.int8)\n",
        "      for i, p in enumerate(preds):\n",
        "        preds_stacked[i] = np.argmax(np.bincount(p))\n",
        "\n",
        "      return preds_stacked\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpTB6lMr00A"
      },
      "source": [
        "---\n",
        "### Классификация изображений\n",
        "\n",
        "Используя введенные выше классы можем перейти уже непосредственно к обучению модели классификации изображений. Пример общего пайплайна решения задачи приведен ниже. Вы можете его расширять и улучшать. В данном примере используются наборы данных 'train_small' и 'test_small'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cTOuZD01Up6",
        "outputId": "b477af89-fa33-48fc-b325-2b4ec5b26600"
      },
      "source": [
        "d_train = Dataset('train_small')\n",
        "d_test = Dataset('test_small')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qd45xXfDwdZjktLFwQb-et-mAaFeCzOR\n",
            "To: /content/train_small.npz\n",
            "100%|██████████| 841M/841M [00:06<00:00, 126MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset train_small from npz.\n",
            "Done. Dataset train_small consists of 7200 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wbRsog0n7uGlHIPGLhyN-PMeT2kdQ2lI\n",
            "To: /content/test_small.npz\n",
            "100%|██████████| 211M/211M [00:01<00:00, 132MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset test_small from npz.\n",
            "Done. Dataset test_small consists of 1800 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBi0XpXg8_wq",
        "scrolled": true
      },
      "source": [
        "model = Model()\n",
        "if not EVALUATE_ONLY:\n",
        "    model.train(d_train)\n",
        "    model.save('best')\n",
        "else:\n",
        "    model = Model.load('best.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcM2EiRMVP93"
      },
      "source": [
        "Пример тестирования модели на части набора данных:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(d_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHgNNbRmRNNv",
        "outputId": "b96e2a1a-71a5-43d3-a138-9838a1314540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "Epoch 17: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3609 - accuracy: 0.9520 - val_loss: 1.2915 - val_accuracy: 0.7600\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.9540\n",
            "Epoch 18: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3715 - accuracy: 0.9540 - val_loss: 0.9317 - val_accuracy: 0.8267\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3697 - accuracy: 0.9540\n",
            "Epoch 19: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3697 - accuracy: 0.9540 - val_loss: 1.0143 - val_accuracy: 0.8000\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.9500\n",
            "Epoch 20: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3818 - accuracy: 0.9500 - val_loss: 1.0899 - val_accuracy: 0.7800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.9240\n",
            "Epoch 21: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4861 - accuracy: 0.9240 - val_loss: 1.6882 - val_accuracy: 0.6067\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4376 - accuracy: 0.9300\n",
            "Epoch 22: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4376 - accuracy: 0.9300 - val_loss: 0.9225 - val_accuracy: 0.8267\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.9440\n",
            "Epoch 23: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3876 - accuracy: 0.9440 - val_loss: 1.0937 - val_accuracy: 0.7867\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.9520\n",
            "Epoch 24: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4107 - accuracy: 0.9520 - val_loss: 1.2508 - val_accuracy: 0.6867\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.9520\n",
            "Epoch 25: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4301 - accuracy: 0.9520 - val_loss: 3.7982 - val_accuracy: 0.4800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.8860\n",
            "Epoch 26: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.5599 - accuracy: 0.8860 - val_loss: 0.9999 - val_accuracy: 0.8000\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.9160\n",
            "Epoch 27: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4806 - accuracy: 0.9160 - val_loss: 1.0057 - val_accuracy: 0.7800\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4658 - accuracy: 0.9260\n",
            "Epoch 28: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4658 - accuracy: 0.9260 - val_loss: 0.8940 - val_accuracy: 0.8067\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.9560\n",
            "Epoch 29: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3802 - accuracy: 0.9560 - val_loss: 1.1265 - val_accuracy: 0.8000\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.9440\n",
            "Epoch 30: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3683 - accuracy: 0.9440 - val_loss: 1.0026 - val_accuracy: 0.7933\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.9400\n",
            "Epoch 31: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3880 - accuracy: 0.9400 - val_loss: 0.9983 - val_accuracy: 0.7867\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9720\n",
            "Epoch 32: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3110 - accuracy: 0.9720 - val_loss: 1.0234 - val_accuracy: 0.8067\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.9740\n",
            "Epoch 33: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3302 - accuracy: 0.9740 - val_loss: 0.9648 - val_accuracy: 0.7733\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9680\n",
            "Epoch 34: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3123 - accuracy: 0.9680 - val_loss: 0.8979 - val_accuracy: 0.8200\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.9640\n",
            "Epoch 35: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3140 - accuracy: 0.9640 - val_loss: 1.2204 - val_accuracy: 0.7533\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9580\n",
            "Epoch 36: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3118 - accuracy: 0.9580 - val_loss: 1.1104 - val_accuracy: 0.7733\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.9740\n",
            "Epoch 37: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3217 - accuracy: 0.9740 - val_loss: 1.0003 - val_accuracy: 0.7867\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.9860\n",
            "Epoch 38: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2700 - accuracy: 0.9860 - val_loss: 1.3955 - val_accuracy: 0.7333\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.9780\n",
            "Epoch 39: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2747 - accuracy: 0.9780 - val_loss: 1.3488 - val_accuracy: 0.7733\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9740\n",
            "Epoch 40: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2779 - accuracy: 0.9740 - val_loss: 1.1616 - val_accuracy: 0.7667\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1129 - accuracy: 0.7620\n",
            "Epoch 1: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.1129 - accuracy: 0.7620 - val_loss: 1.8220 - val_accuracy: 0.6133\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.8200\n",
            "Epoch 2: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.8188 - accuracy: 0.8200 - val_loss: 1.3202 - val_accuracy: 0.6867\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6451 - accuracy: 0.8600\n",
            "Epoch 3: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.6451 - accuracy: 0.8600 - val_loss: 0.7146 - val_accuracy: 0.8333\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4968 - accuracy: 0.9100\n",
            "Epoch 4: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4968 - accuracy: 0.9100 - val_loss: 0.7193 - val_accuracy: 0.8200\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4827 - accuracy: 0.9160\n",
            "Epoch 5: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4827 - accuracy: 0.9160 - val_loss: 0.8020 - val_accuracy: 0.8000\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.9140\n",
            "Epoch 6: val_accuracy did not improve from 0.85333\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4740 - accuracy: 0.9140 - val_loss: 0.7739 - val_accuracy: 0.8400\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4656 - accuracy: 0.9340\n",
            "Epoch 7: val_accuracy improved from 0.85333 to 0.86000, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 10s 649ms/step - loss: 0.4656 - accuracy: 0.9340 - val_loss: 0.7194 - val_accuracy: 0.8600\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.9440\n",
            "Epoch 8: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4190 - accuracy: 0.9440 - val_loss: 0.8059 - val_accuracy: 0.8467\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3873 - accuracy: 0.9520\n",
            "Epoch 9: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3873 - accuracy: 0.9520 - val_loss: 1.0139 - val_accuracy: 0.7933\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3786 - accuracy: 0.9460\n",
            "Epoch 10: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3786 - accuracy: 0.9460 - val_loss: 0.8493 - val_accuracy: 0.8000\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4717 - accuracy: 0.9280\n",
            "Epoch 11: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4717 - accuracy: 0.9280 - val_loss: 0.8395 - val_accuracy: 0.8400\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3874 - accuracy: 0.9480\n",
            "Epoch 12: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3874 - accuracy: 0.9480 - val_loss: 0.9668 - val_accuracy: 0.8133\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.9240\n",
            "Epoch 13: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4032 - accuracy: 0.9240 - val_loss: 0.9524 - val_accuracy: 0.8467\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4044 - accuracy: 0.9260\n",
            "Epoch 14: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4044 - accuracy: 0.9260 - val_loss: 1.9868 - val_accuracy: 0.6467\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3880 - accuracy: 0.9480\n",
            "Epoch 15: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3880 - accuracy: 0.9480 - val_loss: 1.0779 - val_accuracy: 0.7667\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.9460\n",
            "Epoch 16: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4036 - accuracy: 0.9460 - val_loss: 1.5335 - val_accuracy: 0.7267\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.9500\n",
            "Epoch 17: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3856 - accuracy: 0.9500 - val_loss: 1.0602 - val_accuracy: 0.8200\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3834 - accuracy: 0.9520\n",
            "Epoch 18: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3834 - accuracy: 0.9520 - val_loss: 1.1059 - val_accuracy: 0.8000\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3716 - accuracy: 0.9500\n",
            "Epoch 19: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3716 - accuracy: 0.9500 - val_loss: 1.2812 - val_accuracy: 0.7667\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.9640\n",
            "Epoch 20: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3345 - accuracy: 0.9640 - val_loss: 1.4064 - val_accuracy: 0.7667\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3485 - accuracy: 0.9580\n",
            "Epoch 21: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.3485 - accuracy: 0.9580 - val_loss: 1.6432 - val_accuracy: 0.6600\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.9660\n",
            "Epoch 22: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3541 - accuracy: 0.9660 - val_loss: 0.9268 - val_accuracy: 0.8400\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3159 - accuracy: 0.9720\n",
            "Epoch 23: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3159 - accuracy: 0.9720 - val_loss: 1.3039 - val_accuracy: 0.8133\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4375 - accuracy: 0.9280\n",
            "Epoch 24: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4375 - accuracy: 0.9280 - val_loss: 1.3748 - val_accuracy: 0.7600\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.9460\n",
            "Epoch 25: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4318 - accuracy: 0.9460 - val_loss: 1.1012 - val_accuracy: 0.8133\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.9400\n",
            "Epoch 26: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3948 - accuracy: 0.9400 - val_loss: 1.2797 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9480\n",
            "Epoch 27: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3695 - accuracy: 0.9480 - val_loss: 1.2419 - val_accuracy: 0.7933\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.9640\n",
            "Epoch 28: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3501 - accuracy: 0.9640 - val_loss: 1.1816 - val_accuracy: 0.7333\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9640\n",
            "Epoch 29: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3100 - accuracy: 0.9640 - val_loss: 0.8378 - val_accuracy: 0.8600\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9860\n",
            "Epoch 30: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2684 - accuracy: 0.9860 - val_loss: 0.8992 - val_accuracy: 0.8600\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9960\n",
            "Epoch 31: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2411 - accuracy: 0.9960 - val_loss: 1.0640 - val_accuracy: 0.8133\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9780\n",
            "Epoch 32: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2743 - accuracy: 0.9780 - val_loss: 1.2116 - val_accuracy: 0.8000\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9820\n",
            "Epoch 33: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2640 - accuracy: 0.9820 - val_loss: 1.4565 - val_accuracy: 0.7467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.9540\n",
            "Epoch 34: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3879 - accuracy: 0.9540 - val_loss: 1.2801 - val_accuracy: 0.7933\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9640\n",
            "Epoch 35: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3054 - accuracy: 0.9640 - val_loss: 1.3536 - val_accuracy: 0.7733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3131 - accuracy: 0.9720\n",
            "Epoch 36: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3131 - accuracy: 0.9720 - val_loss: 1.1813 - val_accuracy: 0.7867\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9840\n",
            "Epoch 37: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2763 - accuracy: 0.9840 - val_loss: 1.6119 - val_accuracy: 0.7667\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9720\n",
            "Epoch 38: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3183 - accuracy: 0.9720 - val_loss: 1.0708 - val_accuracy: 0.7933\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2588 - accuracy: 0.9820\n",
            "Epoch 39: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2588 - accuracy: 0.9820 - val_loss: 0.9920 - val_accuracy: 0.8267\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.9840\n",
            "Epoch 40: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2708 - accuracy: 0.9840 - val_loss: 1.1747 - val_accuracy: 0.8000\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1203 - accuracy: 0.7860\n",
            "Epoch 1: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 1.1203 - accuracy: 0.7860 - val_loss: 1.2623 - val_accuracy: 0.7200\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9736 - accuracy: 0.7600\n",
            "Epoch 2: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.9736 - accuracy: 0.7600 - val_loss: 1.3364 - val_accuracy: 0.6000\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7603 - accuracy: 0.8300\n",
            "Epoch 3: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.7603 - accuracy: 0.8300 - val_loss: 1.0452 - val_accuracy: 0.7267\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7095 - accuracy: 0.8400\n",
            "Epoch 4: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.7095 - accuracy: 0.8400 - val_loss: 0.8541 - val_accuracy: 0.7733\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.8580\n",
            "Epoch 5: val_accuracy did not improve from 0.86000\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.6367 - accuracy: 0.8580 - val_loss: 0.9357 - val_accuracy: 0.7600\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5769 - accuracy: 0.8720\n",
            "Epoch 6: val_accuracy improved from 0.86000 to 0.88667, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 7s 480ms/step - loss: 0.5769 - accuracy: 0.8720 - val_loss: 0.5771 - val_accuracy: 0.8867\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4976 - accuracy: 0.9080\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4976 - accuracy: 0.9080 - val_loss: 1.0735 - val_accuracy: 0.7733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5127 - accuracy: 0.9060\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.5127 - accuracy: 0.9060 - val_loss: 0.7338 - val_accuracy: 0.8200\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.9320\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.4215 - accuracy: 0.9320 - val_loss: 1.1214 - val_accuracy: 0.8000\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.9260\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4188 - accuracy: 0.9260 - val_loss: 0.9169 - val_accuracy: 0.7533\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.9280\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4259 - accuracy: 0.9280 - val_loss: 0.6764 - val_accuracy: 0.8467\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4512 - accuracy: 0.9040\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4512 - accuracy: 0.9040 - val_loss: 1.2670 - val_accuracy: 0.7800\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.9300\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3961 - accuracy: 0.9300 - val_loss: 1.2249 - val_accuracy: 0.7600\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5355 - accuracy: 0.8940\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.5355 - accuracy: 0.8940 - val_loss: 2.0691 - val_accuracy: 0.5200\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.8980\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4933 - accuracy: 0.8980 - val_loss: 0.7405 - val_accuracy: 0.8400\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3944 - accuracy: 0.9420\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3944 - accuracy: 0.9420 - val_loss: 0.9945 - val_accuracy: 0.8200\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9620\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3357 - accuracy: 0.9620 - val_loss: 0.6474 - val_accuracy: 0.8733\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3022 - accuracy: 0.9760\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3022 - accuracy: 0.9760 - val_loss: 0.8865 - val_accuracy: 0.8267\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9800\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2763 - accuracy: 0.9800 - val_loss: 0.8299 - val_accuracy: 0.8600\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9900\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2684 - accuracy: 0.9900 - val_loss: 0.9703 - val_accuracy: 0.8133\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.9660\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3254 - accuracy: 0.9660 - val_loss: 1.3218 - val_accuracy: 0.7533\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.9740\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.2950 - accuracy: 0.9740 - val_loss: 0.9213 - val_accuracy: 0.8133\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3223 - accuracy: 0.9680\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.3223 - accuracy: 0.9680 - val_loss: 1.1715 - val_accuracy: 0.8067\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.9340\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3853 - accuracy: 0.9340 - val_loss: 1.3730 - val_accuracy: 0.7267\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4157 - accuracy: 0.9300\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4157 - accuracy: 0.9300 - val_loss: 0.8314 - val_accuracy: 0.8467\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.9360\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4529 - accuracy: 0.9360 - val_loss: 0.9651 - val_accuracy: 0.8400\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3815 - accuracy: 0.9500\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3815 - accuracy: 0.9500 - val_loss: 1.5889 - val_accuracy: 0.7667\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.9120\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.5277 - accuracy: 0.9120 - val_loss: 1.2066 - val_accuracy: 0.7667\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.9180\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.5063 - accuracy: 0.9180 - val_loss: 0.7951 - val_accuracy: 0.8600\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.9400\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3969 - accuracy: 0.9400 - val_loss: 1.3833 - val_accuracy: 0.7667\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.9500\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3490 - accuracy: 0.9500 - val_loss: 1.0925 - val_accuracy: 0.7800\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.9920\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2665 - accuracy: 0.9920 - val_loss: 1.1360 - val_accuracy: 0.8067\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9860\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2729 - accuracy: 0.9860 - val_loss: 1.2121 - val_accuracy: 0.8133\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2589 - accuracy: 0.9860\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2589 - accuracy: 0.9860 - val_loss: 0.9193 - val_accuracy: 0.8800\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9760\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2912 - accuracy: 0.9760 - val_loss: 0.8575 - val_accuracy: 0.8867\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.9820\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2757 - accuracy: 0.9820 - val_loss: 1.1573 - val_accuracy: 0.7600\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9760\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2864 - accuracy: 0.9760 - val_loss: 1.1325 - val_accuracy: 0.8067\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.9880\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2594 - accuracy: 0.9880 - val_loss: 0.8888 - val_accuracy: 0.8400\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9960\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2437 - accuracy: 0.9960 - val_loss: 1.1267 - val_accuracy: 0.8000\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9820\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2663 - accuracy: 0.9820 - val_loss: 1.0118 - val_accuracy: 0.8067\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1727 - accuracy: 0.7800\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 1.1727 - accuracy: 0.7800 - val_loss: 1.1618 - val_accuracy: 0.7133\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9112 - accuracy: 0.7860\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.9112 - accuracy: 0.7860 - val_loss: 1.8294 - val_accuracy: 0.5133\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8230 - accuracy: 0.8180\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.8230 - accuracy: 0.8180 - val_loss: 1.2444 - val_accuracy: 0.6867\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6399 - accuracy: 0.8500\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.6399 - accuracy: 0.8500 - val_loss: 0.9746 - val_accuracy: 0.7667\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.9060\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4856 - accuracy: 0.9060 - val_loss: 0.6125 - val_accuracy: 0.8600\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4753 - accuracy: 0.9140\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4753 - accuracy: 0.9140 - val_loss: 0.7665 - val_accuracy: 0.8200\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4698 - accuracy: 0.9060\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4698 - accuracy: 0.9060 - val_loss: 1.2652 - val_accuracy: 0.7733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.9400\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3998 - accuracy: 0.9400 - val_loss: 0.7993 - val_accuracy: 0.8200\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.9300\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4470 - accuracy: 0.9300 - val_loss: 0.7779 - val_accuracy: 0.8267\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.9520\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3580 - accuracy: 0.9520 - val_loss: 0.7153 - val_accuracy: 0.8267\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9560\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3341 - accuracy: 0.9560 - val_loss: 0.7958 - val_accuracy: 0.8133\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.9560\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3472 - accuracy: 0.9560 - val_loss: 0.8412 - val_accuracy: 0.8467\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 0.9500\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.3590 - accuracy: 0.9500 - val_loss: 0.9490 - val_accuracy: 0.7933\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.9620\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3580 - accuracy: 0.9620 - val_loss: 0.7266 - val_accuracy: 0.8400\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.9540\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3501 - accuracy: 0.9540 - val_loss: 0.8126 - val_accuracy: 0.8467\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9520\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3517 - accuracy: 0.9520 - val_loss: 0.9105 - val_accuracy: 0.8333\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9540\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3419 - accuracy: 0.9540 - val_loss: 0.8377 - val_accuracy: 0.8000\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9620\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3123 - accuracy: 0.9620 - val_loss: 1.0936 - val_accuracy: 0.7867\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.9560\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3309 - accuracy: 0.9560 - val_loss: 1.2636 - val_accuracy: 0.7867\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3765 - accuracy: 0.9380\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3765 - accuracy: 0.9380 - val_loss: 0.6690 - val_accuracy: 0.8600\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.9520\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3541 - accuracy: 0.9520 - val_loss: 0.7881 - val_accuracy: 0.8600\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9500\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3695 - accuracy: 0.9500 - val_loss: 0.9464 - val_accuracy: 0.8133\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9460\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4307 - accuracy: 0.9460 - val_loss: 1.4400 - val_accuracy: 0.6867\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.9360\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4255 - accuracy: 0.9360 - val_loss: 0.9284 - val_accuracy: 0.8267\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9640\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3357 - accuracy: 0.9640 - val_loss: 1.0923 - val_accuracy: 0.7933\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.9640\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3377 - accuracy: 0.9640 - val_loss: 0.7512 - val_accuracy: 0.8800\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.9700\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3191 - accuracy: 0.9700 - val_loss: 0.9669 - val_accuracy: 0.8000\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3561 - accuracy: 0.9560\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3561 - accuracy: 0.9560 - val_loss: 1.2925 - val_accuracy: 0.7733\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.9260\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4485 - accuracy: 0.9260 - val_loss: 1.3384 - val_accuracy: 0.7267\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.9440\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4060 - accuracy: 0.9440 - val_loss: 2.5212 - val_accuracy: 0.5667\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.9640\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3438 - accuracy: 0.9640 - val_loss: 0.7938 - val_accuracy: 0.8133\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.9620\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3444 - accuracy: 0.9620 - val_loss: 0.7996 - val_accuracy: 0.8400\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2773 - accuracy: 0.9760\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2773 - accuracy: 0.9760 - val_loss: 0.9234 - val_accuracy: 0.8200\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9880\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2580 - accuracy: 0.9880 - val_loss: 0.8189 - val_accuracy: 0.8600\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9920\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2515 - accuracy: 0.9920 - val_loss: 0.9842 - val_accuracy: 0.8133\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9920\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2539 - accuracy: 0.9920 - val_loss: 1.2693 - val_accuracy: 0.8000\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.9840\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2649 - accuracy: 0.9840 - val_loss: 0.5953 - val_accuracy: 0.8733\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9780\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2973 - accuracy: 0.9780 - val_loss: 1.9445 - val_accuracy: 0.7133\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.9800\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2738 - accuracy: 0.9800 - val_loss: 0.8006 - val_accuracy: 0.8400\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.9720\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3216 - accuracy: 0.9720 - val_loss: 0.8059 - val_accuracy: 0.8200\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0020 - accuracy: 0.7900\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 1.0020 - accuracy: 0.7900 - val_loss: 4.1605 - val_accuracy: 0.4733\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0126 - accuracy: 0.7520\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 1.0126 - accuracy: 0.7520 - val_loss: 2.5768 - val_accuracy: 0.4800\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6995 - accuracy: 0.8520\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.6995 - accuracy: 0.8520 - val_loss: 1.2393 - val_accuracy: 0.7200\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.8880\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5746 - accuracy: 0.8880 - val_loss: 1.1071 - val_accuracy: 0.7200\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8960\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5572 - accuracy: 0.8960 - val_loss: 1.8178 - val_accuracy: 0.6133\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.9160\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4762 - accuracy: 0.9160 - val_loss: 1.4429 - val_accuracy: 0.7200\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4706 - accuracy: 0.9080\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4706 - accuracy: 0.9080 - val_loss: 1.0744 - val_accuracy: 0.7600\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.9280\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4270 - accuracy: 0.9280 - val_loss: 1.0808 - val_accuracy: 0.7867\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.9320\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4144 - accuracy: 0.9320 - val_loss: 1.2444 - val_accuracy: 0.8133\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.9520\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3722 - accuracy: 0.9520 - val_loss: 1.3343 - val_accuracy: 0.7333\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.9780\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3058 - accuracy: 0.9780 - val_loss: 0.8988 - val_accuracy: 0.8267\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3040 - accuracy: 0.9760\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3040 - accuracy: 0.9760 - val_loss: 0.9588 - val_accuracy: 0.8067\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.9700\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2959 - accuracy: 0.9700 - val_loss: 0.9437 - val_accuracy: 0.8267\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.9660\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2872 - accuracy: 0.9660 - val_loss: 1.4269 - val_accuracy: 0.7533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3393 - accuracy: 0.9600\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3393 - accuracy: 0.9600 - val_loss: 0.9570 - val_accuracy: 0.8333\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3572 - accuracy: 0.9460\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3572 - accuracy: 0.9460 - val_loss: 1.0752 - val_accuracy: 0.7933\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3661 - accuracy: 0.9460\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3661 - accuracy: 0.9460 - val_loss: 1.1076 - val_accuracy: 0.7867\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2945 - accuracy: 0.9860\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2945 - accuracy: 0.9860 - val_loss: 1.0457 - val_accuracy: 0.8133\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2802 - accuracy: 0.9720\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2802 - accuracy: 0.9720 - val_loss: 1.3484 - val_accuracy: 0.7533\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.9560\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3371 - accuracy: 0.9560 - val_loss: 1.2308 - val_accuracy: 0.7933\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2868 - accuracy: 0.9700\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2868 - accuracy: 0.9700 - val_loss: 1.2567 - val_accuracy: 0.7933\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.9720\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3085 - accuracy: 0.9720 - val_loss: 0.9078 - val_accuracy: 0.8333\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9780\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2768 - accuracy: 0.9780 - val_loss: 1.0644 - val_accuracy: 0.8133\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9740\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2900 - accuracy: 0.9740 - val_loss: 1.0591 - val_accuracy: 0.8400\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.9720\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2944 - accuracy: 0.9720 - val_loss: 1.0454 - val_accuracy: 0.8200\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9740\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.2844 - accuracy: 0.9740 - val_loss: 1.1252 - val_accuracy: 0.8200\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9760\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2658 - accuracy: 0.9760 - val_loss: 1.1026 - val_accuracy: 0.7733\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9780\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2573 - accuracy: 0.9780 - val_loss: 1.0471 - val_accuracy: 0.8067\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9800\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2789 - accuracy: 0.9800 - val_loss: 2.2095 - val_accuracy: 0.6267\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4577 - accuracy: 0.9360\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4577 - accuracy: 0.9360 - val_loss: 1.3369 - val_accuracy: 0.7333\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3762 - accuracy: 0.9500\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3762 - accuracy: 0.9500 - val_loss: 1.7076 - val_accuracy: 0.7600\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.9560\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3318 - accuracy: 0.9560 - val_loss: 1.5817 - val_accuracy: 0.6867\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9700\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3081 - accuracy: 0.9700 - val_loss: 1.1474 - val_accuracy: 0.7867\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3020 - accuracy: 0.9720\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3020 - accuracy: 0.9720 - val_loss: 1.3841 - val_accuracy: 0.7733\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.9740\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3036 - accuracy: 0.9740 - val_loss: 1.2486 - val_accuracy: 0.7733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9820\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2551 - accuracy: 0.9820 - val_loss: 1.6650 - val_accuracy: 0.7400\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2659 - accuracy: 0.9880\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2659 - accuracy: 0.9880 - val_loss: 1.2391 - val_accuracy: 0.8133\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9880\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2480 - accuracy: 0.9880 - val_loss: 1.8791 - val_accuracy: 0.7000\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9800\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2634 - accuracy: 0.9800 - val_loss: 1.3782 - val_accuracy: 0.7733\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9820\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2769 - accuracy: 0.9820 - val_loss: 1.7672 - val_accuracy: 0.7933\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1898 - accuracy: 0.7580\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 1.1898 - accuracy: 0.7580 - val_loss: 1.3316 - val_accuracy: 0.7000\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7849 - accuracy: 0.8080\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.7849 - accuracy: 0.8080 - val_loss: 1.1386 - val_accuracy: 0.7000\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.8400\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.6612 - accuracy: 0.8400 - val_loss: 0.8952 - val_accuracy: 0.7667\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8960\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5120 - accuracy: 0.8960 - val_loss: 1.4193 - val_accuracy: 0.6933\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.9060\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4777 - accuracy: 0.9060 - val_loss: 0.5820 - val_accuracy: 0.8733\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.9060\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4612 - accuracy: 0.9060 - val_loss: 0.6872 - val_accuracy: 0.8400\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4009 - accuracy: 0.9380\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4009 - accuracy: 0.9380 - val_loss: 1.3864 - val_accuracy: 0.7267\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.9460\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3869 - accuracy: 0.9460 - val_loss: 0.7564 - val_accuracy: 0.7933\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9620\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3429 - accuracy: 0.9620 - val_loss: 1.0974 - val_accuracy: 0.7467\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.9440\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3654 - accuracy: 0.9440 - val_loss: 0.7765 - val_accuracy: 0.8333\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.9560\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3495 - accuracy: 0.9560 - val_loss: 1.2356 - val_accuracy: 0.7467\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5455 - accuracy: 0.9000\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.5455 - accuracy: 0.9000 - val_loss: 1.3570 - val_accuracy: 0.7533\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.9340\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4056 - accuracy: 0.9340 - val_loss: 0.8266 - val_accuracy: 0.8067\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4441 - accuracy: 0.9240\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4441 - accuracy: 0.9240 - val_loss: 1.8724 - val_accuracy: 0.7000\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.9060\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.4839 - accuracy: 0.9060 - val_loss: 0.6847 - val_accuracy: 0.8067\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.9560\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3442 - accuracy: 0.9560 - val_loss: 1.1648 - val_accuracy: 0.7733\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3078 - accuracy: 0.9640\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3078 - accuracy: 0.9640 - val_loss: 0.7586 - val_accuracy: 0.8467\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9860\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2743 - accuracy: 0.9860 - val_loss: 0.7342 - val_accuracy: 0.8333\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.9900\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2498 - accuracy: 0.9900 - val_loss: 0.5558 - val_accuracy: 0.8867\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2348 - accuracy: 0.9920\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2348 - accuracy: 0.9920 - val_loss: 0.6204 - val_accuracy: 0.8800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9740\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2737 - accuracy: 0.9740 - val_loss: 1.1026 - val_accuracy: 0.7533\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9740\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2788 - accuracy: 0.9740 - val_loss: 1.0201 - val_accuracy: 0.7867\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9860\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2509 - accuracy: 0.9860 - val_loss: 0.9794 - val_accuracy: 0.7933\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2314 - accuracy: 0.9900\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.2314 - accuracy: 0.9900 - val_loss: 0.7076 - val_accuracy: 0.8667\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2587 - accuracy: 0.9780\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2587 - accuracy: 0.9780 - val_loss: 0.9158 - val_accuracy: 0.8467\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.9660\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3186 - accuracy: 0.9660 - val_loss: 1.0592 - val_accuracy: 0.8467\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.9600\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3819 - accuracy: 0.9600 - val_loss: 1.5230 - val_accuracy: 0.7600\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.9640\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3375 - accuracy: 0.9640 - val_loss: 1.1225 - val_accuracy: 0.8200\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.9040\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5716 - accuracy: 0.9040 - val_loss: 1.2315 - val_accuracy: 0.7333\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3833 - accuracy: 0.9400\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3833 - accuracy: 0.9400 - val_loss: 0.9442 - val_accuracy: 0.7600\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.9400\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3861 - accuracy: 0.9400 - val_loss: 3.0912 - val_accuracy: 0.5200\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.9660\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3332 - accuracy: 0.9660 - val_loss: 1.2355 - val_accuracy: 0.7667\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2580 - accuracy: 0.9840\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2580 - accuracy: 0.9840 - val_loss: 0.9610 - val_accuracy: 0.8133\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9940\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2368 - accuracy: 0.9940 - val_loss: 1.0397 - val_accuracy: 0.7867\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9900\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2301 - accuracy: 0.9900 - val_loss: 1.1234 - val_accuracy: 0.7933\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2285 - accuracy: 0.9900\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2285 - accuracy: 0.9900 - val_loss: 0.8079 - val_accuracy: 0.8467\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9900\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2487 - accuracy: 0.9900 - val_loss: 0.8305 - val_accuracy: 0.8733\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9820\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2630 - accuracy: 0.9820 - val_loss: 1.2915 - val_accuracy: 0.7733\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9800\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2720 - accuracy: 0.9800 - val_loss: 1.0571 - val_accuracy: 0.7933\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3624 - accuracy: 0.9560\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3624 - accuracy: 0.9560 - val_loss: 1.2095 - val_accuracy: 0.7667\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.5121 - accuracy: 0.6940\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 300ms/step - loss: 1.5121 - accuracy: 0.6940 - val_loss: 1.9810 - val_accuracy: 0.5400\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9703 - accuracy: 0.7500\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.9703 - accuracy: 0.7500 - val_loss: 1.2032 - val_accuracy: 0.7133\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.8000\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.7854 - accuracy: 0.8000 - val_loss: 0.9472 - val_accuracy: 0.7733\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6255 - accuracy: 0.8640\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.6255 - accuracy: 0.8640 - val_loss: 0.9593 - val_accuracy: 0.7733\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5850 - accuracy: 0.8660\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5850 - accuracy: 0.8660 - val_loss: 0.7360 - val_accuracy: 0.8333\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.8940\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.5008 - accuracy: 0.8940 - val_loss: 0.8320 - val_accuracy: 0.7933\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5870 - accuracy: 0.8800\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.5870 - accuracy: 0.8800 - val_loss: 0.9434 - val_accuracy: 0.7667\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.9160\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4682 - accuracy: 0.9160 - val_loss: 0.7817 - val_accuracy: 0.8067\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.9340\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.4241 - accuracy: 0.9340 - val_loss: 0.7162 - val_accuracy: 0.8667\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.9320\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4249 - accuracy: 0.9320 - val_loss: 0.8860 - val_accuracy: 0.8400\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4785 - accuracy: 0.9260\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4785 - accuracy: 0.9260 - val_loss: 0.6988 - val_accuracy: 0.8000\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4081 - accuracy: 0.9340\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4081 - accuracy: 0.9340 - val_loss: 0.8105 - val_accuracy: 0.8667\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.9620\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3245 - accuracy: 0.9620 - val_loss: 0.6710 - val_accuracy: 0.8667\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9640\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3127 - accuracy: 0.9640 - val_loss: 0.8612 - val_accuracy: 0.8333\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.9720\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3121 - accuracy: 0.9720 - val_loss: 1.3234 - val_accuracy: 0.7333\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.9540\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3462 - accuracy: 0.9540 - val_loss: 1.2515 - val_accuracy: 0.7000\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.9680\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3186 - accuracy: 0.9680 - val_loss: 0.9307 - val_accuracy: 0.8333\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3044 - accuracy: 0.9720\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3044 - accuracy: 0.9720 - val_loss: 1.1445 - val_accuracy: 0.7667\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.9660\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3036 - accuracy: 0.9660 - val_loss: 1.0326 - val_accuracy: 0.7733\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.9520\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3883 - accuracy: 0.9520 - val_loss: 1.2893 - val_accuracy: 0.8200\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.9320\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4182 - accuracy: 0.9320 - val_loss: 1.7633 - val_accuracy: 0.6733\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4178 - accuracy: 0.9340\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4178 - accuracy: 0.9340 - val_loss: 1.4836 - val_accuracy: 0.6800\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.9440\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3603 - accuracy: 0.9440 - val_loss: 1.5371 - val_accuracy: 0.7400\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.9800\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2950 - accuracy: 0.9800 - val_loss: 1.4627 - val_accuracy: 0.6800\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2862 - accuracy: 0.9780\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2862 - accuracy: 0.9780 - val_loss: 1.3659 - val_accuracy: 0.7400\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.9840\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2738 - accuracy: 0.9840 - val_loss: 0.8922 - val_accuracy: 0.8200\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9820\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2439 - accuracy: 0.9820 - val_loss: 1.0443 - val_accuracy: 0.8067\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.9640\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3175 - accuracy: 0.9640 - val_loss: 0.9487 - val_accuracy: 0.8400\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9740\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2822 - accuracy: 0.9740 - val_loss: 0.9299 - val_accuracy: 0.7933\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9860\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2409 - accuracy: 0.9860 - val_loss: 1.3028 - val_accuracy: 0.7400\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9860\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2479 - accuracy: 0.9860 - val_loss: 0.9775 - val_accuracy: 0.8467\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.9660\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3208 - accuracy: 0.9660 - val_loss: 1.0619 - val_accuracy: 0.8333\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9820\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2680 - accuracy: 0.9820 - val_loss: 0.8807 - val_accuracy: 0.8467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3236 - accuracy: 0.9640\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3236 - accuracy: 0.9640 - val_loss: 0.8482 - val_accuracy: 0.8333\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.9540\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3453 - accuracy: 0.9540 - val_loss: 2.0612 - val_accuracy: 0.6467\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.9360\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3910 - accuracy: 0.9360 - val_loss: 1.1830 - val_accuracy: 0.7667\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3641 - accuracy: 0.9460\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3641 - accuracy: 0.9460 - val_loss: 1.2072 - val_accuracy: 0.8467\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4058 - accuracy: 0.9460\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4058 - accuracy: 0.9460 - val_loss: 1.0780 - val_accuracy: 0.7800\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.9640\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3443 - accuracy: 0.9640 - val_loss: 1.7602 - val_accuracy: 0.6867\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2871 - accuracy: 0.9720\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2871 - accuracy: 0.9720 - val_loss: 1.3774 - val_accuracy: 0.7467\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0276 - accuracy: 0.7800\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 1.0276 - accuracy: 0.7800 - val_loss: 1.4631 - val_accuracy: 0.6800\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8251 - accuracy: 0.8000\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.8251 - accuracy: 0.8000 - val_loss: 1.1613 - val_accuracy: 0.6533\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6520 - accuracy: 0.8600\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.6520 - accuracy: 0.8600 - val_loss: 1.0062 - val_accuracy: 0.7133\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4671 - accuracy: 0.9160\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4671 - accuracy: 0.9160 - val_loss: 1.1214 - val_accuracy: 0.6867\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4127 - accuracy: 0.9320\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4127 - accuracy: 0.9320 - val_loss: 0.9116 - val_accuracy: 0.7933\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.9260\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4121 - accuracy: 0.9260 - val_loss: 0.7710 - val_accuracy: 0.8200\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3688 - accuracy: 0.9400\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3688 - accuracy: 0.9400 - val_loss: 1.0985 - val_accuracy: 0.7667\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.9600\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3391 - accuracy: 0.9600 - val_loss: 0.9058 - val_accuracy: 0.8133\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.9580\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3473 - accuracy: 0.9580 - val_loss: 1.1213 - val_accuracy: 0.7667\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.9640\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3234 - accuracy: 0.9640 - val_loss: 1.3734 - val_accuracy: 0.7533\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.9480\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4010 - accuracy: 0.9480 - val_loss: 1.4014 - val_accuracy: 0.6867\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.9300\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4166 - accuracy: 0.9300 - val_loss: 1.8838 - val_accuracy: 0.7000\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.9240\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4255 - accuracy: 0.9240 - val_loss: 1.2018 - val_accuracy: 0.7667\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4452 - accuracy: 0.9380\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4452 - accuracy: 0.9380 - val_loss: 1.1501 - val_accuracy: 0.7933\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4352 - accuracy: 0.9320\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4352 - accuracy: 0.9320 - val_loss: 1.6515 - val_accuracy: 0.6533\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3856 - accuracy: 0.9380\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3856 - accuracy: 0.9380 - val_loss: 2.6618 - val_accuracy: 0.5400\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.9720\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3115 - accuracy: 0.9720 - val_loss: 1.0359 - val_accuracy: 0.8000\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2640 - accuracy: 0.9820\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2640 - accuracy: 0.9820 - val_loss: 1.0798 - val_accuracy: 0.8067\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9860\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2555 - accuracy: 0.9860 - val_loss: 1.1951 - val_accuracy: 0.8400\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9800\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3172 - accuracy: 0.9800 - val_loss: 1.0023 - val_accuracy: 0.8667\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2407 - accuracy: 0.9920\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2407 - accuracy: 0.9920 - val_loss: 1.0383 - val_accuracy: 0.8400\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9760\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2632 - accuracy: 0.9760 - val_loss: 1.3145 - val_accuracy: 0.8400\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.9840\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2432 - accuracy: 0.9840 - val_loss: 1.4385 - val_accuracy: 0.8000\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9720\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3145 - accuracy: 0.9720 - val_loss: 1.3574 - val_accuracy: 0.7867\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9720\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2663 - accuracy: 0.9720 - val_loss: 1.3848 - val_accuracy: 0.7800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9860\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2559 - accuracy: 0.9860 - val_loss: 1.4810 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.9680\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2845 - accuracy: 0.9680 - val_loss: 1.1400 - val_accuracy: 0.8333\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9800\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2656 - accuracy: 0.9800 - val_loss: 1.7384 - val_accuracy: 0.7267\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.9600\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3118 - accuracy: 0.9600 - val_loss: 1.2687 - val_accuracy: 0.7933\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.9600\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3492 - accuracy: 0.9600 - val_loss: 1.3784 - val_accuracy: 0.7000\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.9620\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3510 - accuracy: 0.9620 - val_loss: 1.4867 - val_accuracy: 0.7533\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.9680\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3030 - accuracy: 0.9680 - val_loss: 1.2202 - val_accuracy: 0.8000\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.9680\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2972 - accuracy: 0.9680 - val_loss: 1.0498 - val_accuracy: 0.8533\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2934 - accuracy: 0.9660\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2934 - accuracy: 0.9660 - val_loss: 0.8385 - val_accuracy: 0.8467\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3287 - accuracy: 0.9640\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3287 - accuracy: 0.9640 - val_loss: 1.3619 - val_accuracy: 0.8000\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3106 - accuracy: 0.9560\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3106 - accuracy: 0.9560 - val_loss: 1.3521 - val_accuracy: 0.8000\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9660\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3145 - accuracy: 0.9660 - val_loss: 1.1937 - val_accuracy: 0.7933\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.9400\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4060 - accuracy: 0.9400 - val_loss: 1.3768 - val_accuracy: 0.7733\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.9540\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3715 - accuracy: 0.9540 - val_loss: 1.4117 - val_accuracy: 0.7600\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.9620\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3147 - accuracy: 0.9620 - val_loss: 1.1902 - val_accuracy: 0.7867\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0716 - accuracy: 0.7700\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.0716 - accuracy: 0.7700 - val_loss: 0.9053 - val_accuracy: 0.7867\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7520 - accuracy: 0.8320\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.7520 - accuracy: 0.8320 - val_loss: 1.3667 - val_accuracy: 0.6533\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6119 - accuracy: 0.8640\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.6119 - accuracy: 0.8640 - val_loss: 0.7111 - val_accuracy: 0.8333\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4688 - accuracy: 0.9060\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4688 - accuracy: 0.9060 - val_loss: 0.7126 - val_accuracy: 0.8667\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4598 - accuracy: 0.9100\n",
            "Epoch 5: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4598 - accuracy: 0.9100 - val_loss: 0.6349 - val_accuracy: 0.8867\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3666 - accuracy: 0.9480\n",
            "Epoch 6: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3666 - accuracy: 0.9480 - val_loss: 0.6621 - val_accuracy: 0.8667\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3429 - accuracy: 0.9620\n",
            "Epoch 7: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3429 - accuracy: 0.9620 - val_loss: 0.7366 - val_accuracy: 0.8733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.9300\n",
            "Epoch 8: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4200 - accuracy: 0.9300 - val_loss: 2.3072 - val_accuracy: 0.6933\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.9420\n",
            "Epoch 9: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.4067 - accuracy: 0.9420 - val_loss: 1.2274 - val_accuracy: 0.7933\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.9480\n",
            "Epoch 10: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3861 - accuracy: 0.9480 - val_loss: 0.9131 - val_accuracy: 0.8467\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3486 - accuracy: 0.9500\n",
            "Epoch 11: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3486 - accuracy: 0.9500 - val_loss: 0.6932 - val_accuracy: 0.8800\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9600\n",
            "Epoch 12: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3383 - accuracy: 0.9600 - val_loss: 0.7473 - val_accuracy: 0.8667\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3291 - accuracy: 0.9560\n",
            "Epoch 13: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3291 - accuracy: 0.9560 - val_loss: 2.3561 - val_accuracy: 0.6533\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9640\n",
            "Epoch 14: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3409 - accuracy: 0.9640 - val_loss: 0.9534 - val_accuracy: 0.8133\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3280 - accuracy: 0.9600\n",
            "Epoch 15: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3280 - accuracy: 0.9600 - val_loss: 0.8063 - val_accuracy: 0.8600\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9640\n",
            "Epoch 16: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2941 - accuracy: 0.9640 - val_loss: 0.9222 - val_accuracy: 0.8400\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3123 - accuracy: 0.9640\n",
            "Epoch 17: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3123 - accuracy: 0.9640 - val_loss: 0.6276 - val_accuracy: 0.8733\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.9860\n",
            "Epoch 18: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2561 - accuracy: 0.9860 - val_loss: 0.7433 - val_accuracy: 0.8667\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9800\n",
            "Epoch 19: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2571 - accuracy: 0.9800 - val_loss: 0.6479 - val_accuracy: 0.8867\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2571 - accuracy: 0.9760\n",
            "Epoch 20: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2571 - accuracy: 0.9760 - val_loss: 0.6608 - val_accuracy: 0.8667\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9740\n",
            "Epoch 21: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2692 - accuracy: 0.9740 - val_loss: 0.6290 - val_accuracy: 0.8733\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2814 - accuracy: 0.9700\n",
            "Epoch 22: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2814 - accuracy: 0.9700 - val_loss: 0.6894 - val_accuracy: 0.8800\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2544 - accuracy: 0.9820\n",
            "Epoch 23: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2544 - accuracy: 0.9820 - val_loss: 1.1348 - val_accuracy: 0.8067\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3139 - accuracy: 0.9800\n",
            "Epoch 24: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3139 - accuracy: 0.9800 - val_loss: 1.1469 - val_accuracy: 0.8133\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.9380\n",
            "Epoch 25: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4113 - accuracy: 0.9380 - val_loss: 2.8021 - val_accuracy: 0.6800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.9200\n",
            "Epoch 26: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4331 - accuracy: 0.9200 - val_loss: 2.0006 - val_accuracy: 0.7400\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.9240\n",
            "Epoch 27: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4798 - accuracy: 0.9240 - val_loss: 0.9863 - val_accuracy: 0.8067\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.9640\n",
            "Epoch 28: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3343 - accuracy: 0.9640 - val_loss: 0.6755 - val_accuracy: 0.8467\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.9540\n",
            "Epoch 29: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3400 - accuracy: 0.9540 - val_loss: 0.7534 - val_accuracy: 0.8733\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9700\n",
            "Epoch 30: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3024 - accuracy: 0.9700 - val_loss: 0.7132 - val_accuracy: 0.8733\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9780\n",
            "Epoch 31: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2634 - accuracy: 0.9780 - val_loss: 0.9417 - val_accuracy: 0.8200\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2786 - accuracy: 0.9740\n",
            "Epoch 32: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2786 - accuracy: 0.9740 - val_loss: 1.0360 - val_accuracy: 0.8200\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9680\n",
            "Epoch 33: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2942 - accuracy: 0.9680 - val_loss: 1.3495 - val_accuracy: 0.8133\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.9800\n",
            "Epoch 34: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2778 - accuracy: 0.9800 - val_loss: 0.7796 - val_accuracy: 0.8533\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9760\n",
            "Epoch 35: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2838 - accuracy: 0.9760 - val_loss: 0.6445 - val_accuracy: 0.8733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9800\n",
            "Epoch 36: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2768 - accuracy: 0.9800 - val_loss: 0.8583 - val_accuracy: 0.8533\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9820\n",
            "Epoch 37: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2523 - accuracy: 0.9820 - val_loss: 0.8532 - val_accuracy: 0.8667\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2391 - accuracy: 0.9940\n",
            "Epoch 38: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2391 - accuracy: 0.9940 - val_loss: 0.8833 - val_accuracy: 0.8600\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2533 - accuracy: 0.9860\n",
            "Epoch 39: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2533 - accuracy: 0.9860 - val_loss: 0.7453 - val_accuracy: 0.8867\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.9700\n",
            "Epoch 40: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2818 - accuracy: 0.9700 - val_loss: 0.7279 - val_accuracy: 0.8667\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0654 - accuracy: 0.8000\n",
            "Epoch 1: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 1.0654 - accuracy: 0.8000 - val_loss: 1.6709 - val_accuracy: 0.6267\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8139 - accuracy: 0.7980\n",
            "Epoch 2: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.8139 - accuracy: 0.7980 - val_loss: 0.9551 - val_accuracy: 0.8067\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.9020\n",
            "Epoch 3: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.5176 - accuracy: 0.9020 - val_loss: 0.8365 - val_accuracy: 0.8200\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.9340\n",
            "Epoch 4: val_accuracy did not improve from 0.88667\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.4241 - accuracy: 0.9340 - val_loss: 0.6609 - val_accuracy: 0.8733\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.9380\n",
            "Epoch 5: val_accuracy improved from 0.88667 to 0.90000, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 10s 635ms/step - loss: 0.3855 - accuracy: 0.9380 - val_loss: 0.6474 - val_accuracy: 0.9000\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.9560\n",
            "Epoch 6: val_accuracy improved from 0.90000 to 0.90667, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 6s 361ms/step - loss: 0.3511 - accuracy: 0.9560 - val_loss: 0.5886 - val_accuracy: 0.9067\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.9660\n",
            "Epoch 7: val_accuracy did not improve from 0.90667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.3239 - accuracy: 0.9660 - val_loss: 0.6661 - val_accuracy: 0.9067\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.9740\n",
            "Epoch 8: val_accuracy did not improve from 0.90667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2778 - accuracy: 0.9740 - val_loss: 0.6872 - val_accuracy: 0.8867\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9760\n",
            "Epoch 9: val_accuracy did not improve from 0.90667\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.2743 - accuracy: 0.9760 - val_loss: 0.6191 - val_accuracy: 0.9000\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.9840\n",
            "Epoch 10: val_accuracy did not improve from 0.90667\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.2523 - accuracy: 0.9840 - val_loss: 0.6194 - val_accuracy: 0.9000\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9760\n",
            "Epoch 11: val_accuracy did not improve from 0.90667\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2574 - accuracy: 0.9760 - val_loss: 0.6246 - val_accuracy: 0.9000\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9880\n",
            "Epoch 12: val_accuracy improved from 0.90667 to 0.92000, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 9s 566ms/step - loss: 0.2417 - accuracy: 0.9880 - val_loss: 0.5927 - val_accuracy: 0.9200\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9740\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2662 - accuracy: 0.9740 - val_loss: 0.7118 - val_accuracy: 0.8800\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.9600\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3329 - accuracy: 0.9600 - val_loss: 2.0459 - val_accuracy: 0.6600\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9520\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3005 - accuracy: 0.9520 - val_loss: 0.9580 - val_accuracy: 0.8467\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3931 - accuracy: 0.9460\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3931 - accuracy: 0.9460 - val_loss: 1.1377 - val_accuracy: 0.7733\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.9380\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4042 - accuracy: 0.9380 - val_loss: 0.9114 - val_accuracy: 0.8467\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9700\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2816 - accuracy: 0.9700 - val_loss: 0.8187 - val_accuracy: 0.8933\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.9760\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2791 - accuracy: 0.9760 - val_loss: 0.7191 - val_accuracy: 0.9067\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2960 - accuracy: 0.9620\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2960 - accuracy: 0.9620 - val_loss: 1.0816 - val_accuracy: 0.8000\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9720\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2914 - accuracy: 0.9720 - val_loss: 0.8215 - val_accuracy: 0.8733\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.9580\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3049 - accuracy: 0.9580 - val_loss: 1.1782 - val_accuracy: 0.8067\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9800\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2680 - accuracy: 0.9800 - val_loss: 1.5854 - val_accuracy: 0.7467\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3773 - accuracy: 0.9520\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3773 - accuracy: 0.9520 - val_loss: 1.9856 - val_accuracy: 0.6867\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.9620\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3401 - accuracy: 0.9620 - val_loss: 1.8981 - val_accuracy: 0.6800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.9560\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3083 - accuracy: 0.9560 - val_loss: 0.7118 - val_accuracy: 0.8800\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2513 - accuracy: 0.9920 - val_loss: 1.0470 - val_accuracy: 0.7733\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9800\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2508 - accuracy: 0.9800 - val_loss: 1.1208 - val_accuracy: 0.7933\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9720\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2842 - accuracy: 0.9720 - val_loss: 0.7808 - val_accuracy: 0.8800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9760\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2793 - accuracy: 0.9760 - val_loss: 1.5394 - val_accuracy: 0.7400\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.9760\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2855 - accuracy: 0.9760 - val_loss: 2.9282 - val_accuracy: 0.5733\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.9480\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3810 - accuracy: 0.9480 - val_loss: 1.3030 - val_accuracy: 0.7800\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3998 - accuracy: 0.9440\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3998 - accuracy: 0.9440 - val_loss: 1.3308 - val_accuracy: 0.7333\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9800\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2698 - accuracy: 0.9800 - val_loss: 1.5637 - val_accuracy: 0.7200\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.9860\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2517 - accuracy: 0.9860 - val_loss: 1.0744 - val_accuracy: 0.8067\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.9800\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2769 - accuracy: 0.9800 - val_loss: 1.2347 - val_accuracy: 0.7533\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9920\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2299 - accuracy: 0.9920 - val_loss: 1.1318 - val_accuracy: 0.7733\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9660\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3102 - accuracy: 0.9660 - val_loss: 0.7467 - val_accuracy: 0.8467\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9940\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2250 - accuracy: 0.9940 - val_loss: 1.0900 - val_accuracy: 0.8200\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9880\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2390 - accuracy: 0.9880 - val_loss: 1.8772 - val_accuracy: 0.7133\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.8040\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.9120 - accuracy: 0.8040 - val_loss: 1.6384 - val_accuracy: 0.6400\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8100 - accuracy: 0.8100\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.8100 - accuracy: 0.8100 - val_loss: 0.9553 - val_accuracy: 0.7267\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7053 - accuracy: 0.8620\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.7053 - accuracy: 0.8620 - val_loss: 0.8055 - val_accuracy: 0.8067\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.8840\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.5213 - accuracy: 0.8840 - val_loss: 1.3165 - val_accuracy: 0.7133\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.9200\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4626 - accuracy: 0.9200 - val_loss: 1.0485 - val_accuracy: 0.7467\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.9340\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3935 - accuracy: 0.9340 - val_loss: 1.3371 - val_accuracy: 0.7533\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3550 - accuracy: 0.9500\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3550 - accuracy: 0.9500 - val_loss: 0.7563 - val_accuracy: 0.8067\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.9480\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3596 - accuracy: 0.9480 - val_loss: 1.0899 - val_accuracy: 0.7733\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9620\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 306ms/step - loss: 0.3100 - accuracy: 0.9620 - val_loss: 0.9720 - val_accuracy: 0.7933\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.9580\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3365 - accuracy: 0.9580 - val_loss: 2.1121 - val_accuracy: 0.6933\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.9460\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3780 - accuracy: 0.9460 - val_loss: 0.9222 - val_accuracy: 0.7667\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3814 - accuracy: 0.9500\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3814 - accuracy: 0.9500 - val_loss: 1.0189 - val_accuracy: 0.7933\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9620\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3409 - accuracy: 0.9620 - val_loss: 0.7982 - val_accuracy: 0.8000\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.9720\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3074 - accuracy: 0.9720 - val_loss: 0.5926 - val_accuracy: 0.8867\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2758 - accuracy: 0.9720\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2758 - accuracy: 0.9720 - val_loss: 0.8178 - val_accuracy: 0.8000\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9680\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2857 - accuracy: 0.9680 - val_loss: 0.7508 - val_accuracy: 0.7867\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9720\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2723 - accuracy: 0.9720 - val_loss: 0.9886 - val_accuracy: 0.7867\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.9620\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3093 - accuracy: 0.9620 - val_loss: 1.4460 - val_accuracy: 0.7533\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3996 - accuracy: 0.9440\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3996 - accuracy: 0.9440 - val_loss: 1.3735 - val_accuracy: 0.7600\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3170 - accuracy: 0.9600\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3170 - accuracy: 0.9600 - val_loss: 0.8116 - val_accuracy: 0.8267\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2973 - accuracy: 0.9600\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2973 - accuracy: 0.9600 - val_loss: 1.4078 - val_accuracy: 0.7000\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.9620\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3004 - accuracy: 0.9620 - val_loss: 1.2727 - val_accuracy: 0.7667\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9800\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2672 - accuracy: 0.9800 - val_loss: 1.4301 - val_accuracy: 0.7333\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2439 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2439 - accuracy: 0.9880 - val_loss: 1.3600 - val_accuracy: 0.7467\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9800\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2436 - accuracy: 0.9800 - val_loss: 1.2106 - val_accuracy: 0.7800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2228 - accuracy: 0.9920\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2228 - accuracy: 0.9920 - val_loss: 0.9267 - val_accuracy: 0.8400\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9960\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2114 - accuracy: 0.9960 - val_loss: 0.8157 - val_accuracy: 0.8400\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9920\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2090 - accuracy: 0.9920 - val_loss: 0.8009 - val_accuracy: 0.8333\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9960\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2051 - accuracy: 0.9960 - val_loss: 0.9179 - val_accuracy: 0.8133\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9840\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2510 - accuracy: 0.9840 - val_loss: 0.8674 - val_accuracy: 0.7733\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2881 - accuracy: 0.9680\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2881 - accuracy: 0.9680 - val_loss: 1.7025 - val_accuracy: 0.7200\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.9620\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3191 - accuracy: 0.9620 - val_loss: 1.4092 - val_accuracy: 0.7400\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.9380\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3824 - accuracy: 0.9380 - val_loss: 1.7147 - val_accuracy: 0.7200\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3658 - accuracy: 0.9440\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3658 - accuracy: 0.9440 - val_loss: 1.1092 - val_accuracy: 0.7733\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.9660\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3342 - accuracy: 0.9660 - val_loss: 1.2900 - val_accuracy: 0.7267\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.9640\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3261 - accuracy: 0.9640 - val_loss: 0.7068 - val_accuracy: 0.8267\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.9460\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3939 - accuracy: 0.9460 - val_loss: 1.6632 - val_accuracy: 0.6600\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.9440\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3845 - accuracy: 0.9440 - val_loss: 2.3238 - val_accuracy: 0.7133\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.9420\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3564 - accuracy: 0.9420 - val_loss: 1.1157 - val_accuracy: 0.7333\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3061 - accuracy: 0.9740\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3061 - accuracy: 0.9740 - val_loss: 1.2340 - val_accuracy: 0.7200\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9825 - accuracy: 0.7940\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 0.9825 - accuracy: 0.7940 - val_loss: 0.8114 - val_accuracy: 0.7933\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.8740\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.6040 - accuracy: 0.8740 - val_loss: 0.9065 - val_accuracy: 0.7667\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.8940\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.5164 - accuracy: 0.8940 - val_loss: 1.0927 - val_accuracy: 0.7467\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.9360\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4293 - accuracy: 0.9360 - val_loss: 1.1198 - val_accuracy: 0.7400\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.9120\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.4826 - accuracy: 0.9120 - val_loss: 2.1255 - val_accuracy: 0.6400\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.9480\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3802 - accuracy: 0.9480 - val_loss: 2.0479 - val_accuracy: 0.6600\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3349 - accuracy: 0.9540\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.3349 - accuracy: 0.9540 - val_loss: 0.8369 - val_accuracy: 0.8267\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3152 - accuracy: 0.9620\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.3152 - accuracy: 0.9620 - val_loss: 0.8897 - val_accuracy: 0.8000\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3051 - accuracy: 0.9660\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3051 - accuracy: 0.9660 - val_loss: 0.7876 - val_accuracy: 0.8400\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2719 - accuracy: 0.9800\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2719 - accuracy: 0.9800 - val_loss: 1.3634 - val_accuracy: 0.7200\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9800\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2515 - accuracy: 0.9800 - val_loss: 0.9449 - val_accuracy: 0.8200\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9920\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2456 - accuracy: 0.9920 - val_loss: 1.0973 - val_accuracy: 0.8067\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9680\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2823 - accuracy: 0.9680 - val_loss: 1.4899 - val_accuracy: 0.7667\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.9880\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2398 - accuracy: 0.9880 - val_loss: 1.1882 - val_accuracy: 0.7800\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9880\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2323 - accuracy: 0.9880 - val_loss: 1.2809 - val_accuracy: 0.7933\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.9640\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2933 - accuracy: 0.9640 - val_loss: 0.9922 - val_accuracy: 0.8267\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.9740\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2602 - accuracy: 0.9740 - val_loss: 1.0288 - val_accuracy: 0.7667\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9560\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2890 - accuracy: 0.9560 - val_loss: 1.3051 - val_accuracy: 0.7533\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4023 - accuracy: 0.9340\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4023 - accuracy: 0.9340 - val_loss: 1.3276 - val_accuracy: 0.7267\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.9400\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.4211 - accuracy: 0.9400 - val_loss: 1.0065 - val_accuracy: 0.7667\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.9560\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3472 - accuracy: 0.9560 - val_loss: 1.4787 - val_accuracy: 0.7800\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9740\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2615 - accuracy: 0.9740 - val_loss: 1.4399 - val_accuracy: 0.6867\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2504 - accuracy: 0.9960\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2504 - accuracy: 0.9960 - val_loss: 1.2363 - val_accuracy: 0.7400\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2496 - accuracy: 0.9880 - val_loss: 1.3985 - val_accuracy: 0.7667\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9900\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2181 - accuracy: 0.9900 - val_loss: 1.2235 - val_accuracy: 0.8000\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9940\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2103 - accuracy: 0.9940 - val_loss: 1.1924 - val_accuracy: 0.8067\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2131 - accuracy: 0.9920 - val_loss: 1.1750 - val_accuracy: 0.8067\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9860\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2310 - accuracy: 0.9860 - val_loss: 1.0156 - val_accuracy: 0.8333\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.9820\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2370 - accuracy: 0.9820 - val_loss: 1.0598 - val_accuracy: 0.7867\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2764 - accuracy: 0.9740\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2764 - accuracy: 0.9740 - val_loss: 1.4516 - val_accuracy: 0.7333\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2652 - accuracy: 0.9760\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2652 - accuracy: 0.9760 - val_loss: 1.2513 - val_accuracy: 0.7867\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9620\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3341 - accuracy: 0.9620 - val_loss: 1.3778 - val_accuracy: 0.7667\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3006 - accuracy: 0.9680\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3006 - accuracy: 0.9680 - val_loss: 1.1330 - val_accuracy: 0.7667\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.9760\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2804 - accuracy: 0.9760 - val_loss: 1.0035 - val_accuracy: 0.8067\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.9640\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3027 - accuracy: 0.9640 - val_loss: 1.0548 - val_accuracy: 0.7800\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.9740\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2794 - accuracy: 0.9740 - val_loss: 0.9490 - val_accuracy: 0.8067\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9780\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2620 - accuracy: 0.9780 - val_loss: 1.1974 - val_accuracy: 0.7467\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2197 - accuracy: 0.9900\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2197 - accuracy: 0.9900 - val_loss: 0.7729 - val_accuracy: 0.8667\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9920\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2235 - accuracy: 0.9920 - val_loss: 0.8367 - val_accuracy: 0.8600\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9920\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2059 - accuracy: 0.9920 - val_loss: 0.8937 - val_accuracy: 0.8400\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0639 - accuracy: 0.8140\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 1.0639 - accuracy: 0.8140 - val_loss: 0.9549 - val_accuracy: 0.7733\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.8360\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.7030 - accuracy: 0.8360 - val_loss: 1.1180 - val_accuracy: 0.7000\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.8800\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.5766 - accuracy: 0.8800 - val_loss: 0.8294 - val_accuracy: 0.7467\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.8920\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.5080 - accuracy: 0.8920 - val_loss: 0.6607 - val_accuracy: 0.8533\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.9220\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4320 - accuracy: 0.9220 - val_loss: 0.8437 - val_accuracy: 0.8133\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4070 - accuracy: 0.9220\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4070 - accuracy: 0.9220 - val_loss: 0.6847 - val_accuracy: 0.8267\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3645 - accuracy: 0.9380\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3645 - accuracy: 0.9380 - val_loss: 1.0031 - val_accuracy: 0.7733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.9600\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3231 - accuracy: 0.9600 - val_loss: 0.8942 - val_accuracy: 0.7867\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.9580\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3481 - accuracy: 0.9580 - val_loss: 0.9025 - val_accuracy: 0.7933\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9700\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2763 - accuracy: 0.9700 - val_loss: 0.7760 - val_accuracy: 0.8467\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9740\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2574 - accuracy: 0.9740 - val_loss: 0.8108 - val_accuracy: 0.8600\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9760\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2880 - accuracy: 0.9760 - val_loss: 1.7328 - val_accuracy: 0.7333\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.9680\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2794 - accuracy: 0.9680 - val_loss: 1.1482 - val_accuracy: 0.8400\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.9520\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3636 - accuracy: 0.9520 - val_loss: 0.6793 - val_accuracy: 0.8800\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3321 - accuracy: 0.9540\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3321 - accuracy: 0.9540 - val_loss: 0.7822 - val_accuracy: 0.8333\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9560\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3073 - accuracy: 0.9560 - val_loss: 0.9085 - val_accuracy: 0.8000\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3575 - accuracy: 0.9580\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.3575 - accuracy: 0.9580 - val_loss: 1.0251 - val_accuracy: 0.8200\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9740\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2686 - accuracy: 0.9740 - val_loss: 0.9864 - val_accuracy: 0.7800\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9900\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2326 - accuracy: 0.9900 - val_loss: 0.8732 - val_accuracy: 0.8067\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9860\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2282 - accuracy: 0.9860 - val_loss: 0.8661 - val_accuracy: 0.8133\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9940\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2097 - accuracy: 0.9940 - val_loss: 0.9180 - val_accuracy: 0.8267\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9940\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2157 - accuracy: 0.9940 - val_loss: 0.9703 - val_accuracy: 0.8400\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9860\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2254 - accuracy: 0.9860 - val_loss: 0.9472 - val_accuracy: 0.8333\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.9840\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2392 - accuracy: 0.9840 - val_loss: 1.0815 - val_accuracy: 0.8200\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9900\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2144 - accuracy: 0.9900 - val_loss: 1.4930 - val_accuracy: 0.7600\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9800\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2343 - accuracy: 0.9800 - val_loss: 1.2014 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.9540\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.3452 - accuracy: 0.9540 - val_loss: 1.5154 - val_accuracy: 0.7533\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3207 - accuracy: 0.9600\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3207 - accuracy: 0.9600 - val_loss: 1.7832 - val_accuracy: 0.7133\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.9660\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3136 - accuracy: 0.9660 - val_loss: 1.0887 - val_accuracy: 0.7800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.9580\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3269 - accuracy: 0.9580 - val_loss: 1.7494 - val_accuracy: 0.6533\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2765 - accuracy: 0.9680\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2765 - accuracy: 0.9680 - val_loss: 1.2387 - val_accuracy: 0.7800\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.9640\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3005 - accuracy: 0.9640 - val_loss: 1.1967 - val_accuracy: 0.7333\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.9720\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2964 - accuracy: 0.9720 - val_loss: 1.2665 - val_accuracy: 0.7867\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9760\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2474 - accuracy: 0.9760 - val_loss: 0.7716 - val_accuracy: 0.8200\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2404 - accuracy: 0.9820\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2404 - accuracy: 0.9820 - val_loss: 1.1960 - val_accuracy: 0.7733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9920\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2217 - accuracy: 0.9920 - val_loss: 0.9478 - val_accuracy: 0.8467\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9820\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2415 - accuracy: 0.9820 - val_loss: 1.6749 - val_accuracy: 0.7733\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9800\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2325 - accuracy: 0.9800 - val_loss: 1.5949 - val_accuracy: 0.7467\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9900\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2133 - accuracy: 0.9900 - val_loss: 1.4181 - val_accuracy: 0.7733\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9920\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2257 - accuracy: 0.9920 - val_loss: 1.1200 - val_accuracy: 0.7933\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1181 - accuracy: 0.8020\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 1.1181 - accuracy: 0.8020 - val_loss: 1.8789 - val_accuracy: 0.5467\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8261 - accuracy: 0.8060\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.8261 - accuracy: 0.8060 - val_loss: 0.9124 - val_accuracy: 0.7000\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.8840\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.5338 - accuracy: 0.8840 - val_loss: 0.8022 - val_accuracy: 0.7533\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.9040\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.4837 - accuracy: 0.9040 - val_loss: 0.6950 - val_accuracy: 0.8333\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.9240\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.4321 - accuracy: 0.9240 - val_loss: 0.6599 - val_accuracy: 0.8733\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3891 - accuracy: 0.9400\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.3891 - accuracy: 0.9400 - val_loss: 0.6939 - val_accuracy: 0.8200\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.9180\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 299ms/step - loss: 0.4183 - accuracy: 0.9180 - val_loss: 1.3415 - val_accuracy: 0.7400\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.9480\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.3455 - accuracy: 0.9480 - val_loss: 1.4150 - val_accuracy: 0.7400\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.9560\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.3124 - accuracy: 0.9560 - val_loss: 0.6750 - val_accuracy: 0.8733\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2904 - accuracy: 0.9700\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2904 - accuracy: 0.9700 - val_loss: 0.6792 - val_accuracy: 0.8667\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9640\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2876 - accuracy: 0.9640 - val_loss: 0.7761 - val_accuracy: 0.8333\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.9780\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2735 - accuracy: 0.9780 - val_loss: 0.6494 - val_accuracy: 0.8600\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2329 - accuracy: 0.9840\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2329 - accuracy: 0.9840 - val_loss: 0.8454 - val_accuracy: 0.8400\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9900\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2190 - accuracy: 0.9900 - val_loss: 0.7664 - val_accuracy: 0.8533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9800\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2390 - accuracy: 0.9800 - val_loss: 1.2080 - val_accuracy: 0.8133\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.9640\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2836 - accuracy: 0.9640 - val_loss: 1.7340 - val_accuracy: 0.7200\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9760\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2510 - accuracy: 0.9760 - val_loss: 1.0417 - val_accuracy: 0.7867\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9660\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2913 - accuracy: 0.9660 - val_loss: 2.0911 - val_accuracy: 0.6667\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.9280\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.4353 - accuracy: 0.9280 - val_loss: 1.1396 - val_accuracy: 0.7867\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3825 - accuracy: 0.9480\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3825 - accuracy: 0.9480 - val_loss: 1.4450 - val_accuracy: 0.7200\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.9640\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3176 - accuracy: 0.9640 - val_loss: 1.0809 - val_accuracy: 0.7800\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.9740\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2678 - accuracy: 0.9740 - val_loss: 1.3494 - val_accuracy: 0.7267\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9760\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2730 - accuracy: 0.9760 - val_loss: 0.8092 - val_accuracy: 0.8667\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9920\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2159 - accuracy: 0.9920 - val_loss: 0.8442 - val_accuracy: 0.8000\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9920\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2113 - accuracy: 0.9920 - val_loss: 0.9591 - val_accuracy: 0.8133\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9920\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2056 - accuracy: 0.9920 - val_loss: 0.7642 - val_accuracy: 0.8600\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9960\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2006 - accuracy: 0.9960 - val_loss: 0.6840 - val_accuracy: 0.8667\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2903 - accuracy: 0.9820\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2903 - accuracy: 0.9820 - val_loss: 1.5078 - val_accuracy: 0.7067\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9760\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2387 - accuracy: 0.9760 - val_loss: 1.3132 - val_accuracy: 0.7733\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9840\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2427 - accuracy: 0.9840 - val_loss: 0.8845 - val_accuracy: 0.8200\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9820\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2482 - accuracy: 0.9820 - val_loss: 1.2293 - val_accuracy: 0.8067\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2742 - accuracy: 0.9720\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2742 - accuracy: 0.9720 - val_loss: 1.8223 - val_accuracy: 0.7000\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9700\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2657 - accuracy: 0.9700 - val_loss: 1.9497 - val_accuracy: 0.6067\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9660\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2717 - accuracy: 0.9660 - val_loss: 1.1362 - val_accuracy: 0.7800\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2575 - accuracy: 0.9780\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2575 - accuracy: 0.9780 - val_loss: 1.1015 - val_accuracy: 0.7600\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9860\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2478 - accuracy: 0.9860 - val_loss: 1.4305 - val_accuracy: 0.7200\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2303 - accuracy: 0.9820\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2303 - accuracy: 0.9820 - val_loss: 1.5727 - val_accuracy: 0.7600\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9900\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2039 - accuracy: 0.9900 - val_loss: 1.0232 - val_accuracy: 0.8333\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9920\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2058 - accuracy: 0.9920 - val_loss: 0.9688 - val_accuracy: 0.8267\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9920\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1955 - accuracy: 0.9920 - val_loss: 0.9804 - val_accuracy: 0.8267\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3159 - accuracy: 0.7480\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 1.3159 - accuracy: 0.7480 - val_loss: 1.8002 - val_accuracy: 0.5333\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0549 - accuracy: 0.7220\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 1.0549 - accuracy: 0.7220 - val_loss: 1.1167 - val_accuracy: 0.6600\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7056 - accuracy: 0.8460\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.7056 - accuracy: 0.8460 - val_loss: 1.0031 - val_accuracy: 0.7267\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5037 - accuracy: 0.9020\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.5037 - accuracy: 0.9020 - val_loss: 0.9413 - val_accuracy: 0.7867\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4541 - accuracy: 0.9200\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4541 - accuracy: 0.9200 - val_loss: 0.9948 - val_accuracy: 0.7933\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4492 - accuracy: 0.9180\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.4492 - accuracy: 0.9180 - val_loss: 1.2862 - val_accuracy: 0.7067\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3524 - accuracy: 0.9440\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3524 - accuracy: 0.9440 - val_loss: 0.9904 - val_accuracy: 0.8133\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9720\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2883 - accuracy: 0.9720 - val_loss: 0.8197 - val_accuracy: 0.8467\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2911 - accuracy: 0.9640\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2911 - accuracy: 0.9640 - val_loss: 1.6835 - val_accuracy: 0.6867\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.9500\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.4102 - accuracy: 0.9500 - val_loss: 1.3100 - val_accuracy: 0.7400\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.9160\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4768 - accuracy: 0.9160 - val_loss: 0.7863 - val_accuracy: 0.8267\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3625 - accuracy: 0.9440\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3625 - accuracy: 0.9440 - val_loss: 0.7890 - val_accuracy: 0.8400\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.9640\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2975 - accuracy: 0.9640 - val_loss: 0.9249 - val_accuracy: 0.8333\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.9720\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2859 - accuracy: 0.9720 - val_loss: 0.9968 - val_accuracy: 0.8200\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9800\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2605 - accuracy: 0.9800 - val_loss: 1.0093 - val_accuracy: 0.8200\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9720\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2908 - accuracy: 0.9720 - val_loss: 1.2995 - val_accuracy: 0.8133\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3879 - accuracy: 0.9400\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3879 - accuracy: 0.9400 - val_loss: 0.9398 - val_accuracy: 0.8267\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9760\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2657 - accuracy: 0.9760 - val_loss: 1.7504 - val_accuracy: 0.7400\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.9700\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3088 - accuracy: 0.9700 - val_loss: 1.4709 - val_accuracy: 0.6933\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9680\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2930 - accuracy: 0.9680 - val_loss: 1.3711 - val_accuracy: 0.7667\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2187 - accuracy: 0.9940\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2187 - accuracy: 0.9940 - val_loss: 1.3021 - val_accuracy: 0.7867\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9840\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2240 - accuracy: 0.9840 - val_loss: 1.1328 - val_accuracy: 0.8067\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9860\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2597 - accuracy: 0.9860 - val_loss: 1.1419 - val_accuracy: 0.8000\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9920\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2185 - accuracy: 0.9920 - val_loss: 1.0701 - val_accuracy: 0.8200\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9920\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2119 - accuracy: 0.9920 - val_loss: 1.0959 - val_accuracy: 0.8400\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2070 - accuracy: 0.9880\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2070 - accuracy: 0.9880 - val_loss: 1.1575 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9900\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2079 - accuracy: 0.9900 - val_loss: 1.2250 - val_accuracy: 0.8067\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9920\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2113 - accuracy: 0.9920 - val_loss: 1.3403 - val_accuracy: 0.8200\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9840\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2231 - accuracy: 0.9840 - val_loss: 1.4931 - val_accuracy: 0.8067\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.9600\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3296 - accuracy: 0.9600 - val_loss: 2.1611 - val_accuracy: 0.6867\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2746 - accuracy: 0.9620\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2746 - accuracy: 0.9620 - val_loss: 1.2570 - val_accuracy: 0.8000\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9720\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2625 - accuracy: 0.9720 - val_loss: 1.2099 - val_accuracy: 0.7600\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9700\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2932 - accuracy: 0.9700 - val_loss: 1.1852 - val_accuracy: 0.8000\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.9700\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3250 - accuracy: 0.9700 - val_loss: 1.3745 - val_accuracy: 0.7667\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.9740\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2724 - accuracy: 0.9740 - val_loss: 2.2582 - val_accuracy: 0.6600\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2399 - accuracy: 0.9820\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2399 - accuracy: 0.9820 - val_loss: 2.0242 - val_accuracy: 0.7200\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9820\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2307 - accuracy: 0.9820 - val_loss: 1.7762 - val_accuracy: 0.7067\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9960\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2018 - accuracy: 0.9960 - val_loss: 1.1511 - val_accuracy: 0.7933\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9920\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2024 - accuracy: 0.9920 - val_loss: 1.0669 - val_accuracy: 0.8200\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9940\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2093 - accuracy: 0.9940 - val_loss: 1.2244 - val_accuracy: 0.8067\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1516 - accuracy: 0.8080\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 1.1516 - accuracy: 0.8080 - val_loss: 2.6904 - val_accuracy: 0.4000\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8563 - accuracy: 0.7780\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.8563 - accuracy: 0.7780 - val_loss: 1.2847 - val_accuracy: 0.6200\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5493 - accuracy: 0.8920\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.5493 - accuracy: 0.8920 - val_loss: 0.9816 - val_accuracy: 0.7600\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.9160\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.4743 - accuracy: 0.9160 - val_loss: 0.9174 - val_accuracy: 0.7800\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.9340\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3808 - accuracy: 0.9340 - val_loss: 0.8990 - val_accuracy: 0.7933\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.9400\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3792 - accuracy: 0.9400 - val_loss: 0.8571 - val_accuracy: 0.8467\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.9500\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3326 - accuracy: 0.9500 - val_loss: 0.7575 - val_accuracy: 0.8667\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.9580\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3278 - accuracy: 0.9580 - val_loss: 0.8628 - val_accuracy: 0.8600\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9720\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2692 - accuracy: 0.9720 - val_loss: 0.8798 - val_accuracy: 0.8000\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.9720\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2772 - accuracy: 0.9720 - val_loss: 0.8234 - val_accuracy: 0.8333\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2664 - accuracy: 0.9680\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2664 - accuracy: 0.9680 - val_loss: 0.8275 - val_accuracy: 0.8267\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9740\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2717 - accuracy: 0.9740 - val_loss: 0.8920 - val_accuracy: 0.8400\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.9540\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3342 - accuracy: 0.9540 - val_loss: 1.0664 - val_accuracy: 0.7733\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3452 - accuracy: 0.9520\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3452 - accuracy: 0.9520 - val_loss: 1.0112 - val_accuracy: 0.8267\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9600\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3102 - accuracy: 0.9600 - val_loss: 1.1990 - val_accuracy: 0.7733\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2900 - accuracy: 0.9740\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2900 - accuracy: 0.9740 - val_loss: 1.0065 - val_accuracy: 0.8067\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.9680\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2783 - accuracy: 0.9680 - val_loss: 1.0629 - val_accuracy: 0.8000\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.9460\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3793 - accuracy: 0.9460 - val_loss: 1.3594 - val_accuracy: 0.7133\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4126 - accuracy: 0.9340\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4126 - accuracy: 0.9340 - val_loss: 2.4973 - val_accuracy: 0.6400\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3573 - accuracy: 0.9380\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3573 - accuracy: 0.9380 - val_loss: 1.2286 - val_accuracy: 0.7800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.9580\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3025 - accuracy: 0.9580 - val_loss: 1.1887 - val_accuracy: 0.7667\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2450 - accuracy: 0.9840\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2450 - accuracy: 0.9840 - val_loss: 0.9857 - val_accuracy: 0.8067\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2211 - accuracy: 0.9900\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2211 - accuracy: 0.9900 - val_loss: 0.8245 - val_accuracy: 0.8533\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2190 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2190 - accuracy: 0.9880 - val_loss: 1.0113 - val_accuracy: 0.8267\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2346 - accuracy: 0.9820\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2346 - accuracy: 0.9820 - val_loss: 1.0096 - val_accuracy: 0.8267\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2279 - accuracy: 0.9920\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2279 - accuracy: 0.9920 - val_loss: 1.0085 - val_accuracy: 0.8333\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2128 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2128 - accuracy: 0.9920 - val_loss: 0.8652 - val_accuracy: 0.8533\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2069 - accuracy: 0.9900\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2069 - accuracy: 0.9900 - val_loss: 0.8764 - val_accuracy: 0.8333\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9940\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1983 - accuracy: 0.9940 - val_loss: 0.8867 - val_accuracy: 0.8733\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9900\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2053 - accuracy: 0.9900 - val_loss: 0.8366 - val_accuracy: 0.8800\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2497 - accuracy: 0.9760\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2497 - accuracy: 0.9760 - val_loss: 1.4344 - val_accuracy: 0.7600\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9740\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2625 - accuracy: 0.9740 - val_loss: 2.4035 - val_accuracy: 0.7000\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.9600\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3451 - accuracy: 0.9600 - val_loss: 1.6179 - val_accuracy: 0.7467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.9320\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3687 - accuracy: 0.9320 - val_loss: 1.6732 - val_accuracy: 0.7333\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9620\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3183 - accuracy: 0.9620 - val_loss: 1.7659 - val_accuracy: 0.7400\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3537 - accuracy: 0.9500\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3537 - accuracy: 0.9500 - val_loss: 1.7092 - val_accuracy: 0.7133\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2996 - accuracy: 0.9640\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2996 - accuracy: 0.9640 - val_loss: 1.3397 - val_accuracy: 0.7600\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.9680\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3245 - accuracy: 0.9680 - val_loss: 1.0904 - val_accuracy: 0.7933\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9600\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3346 - accuracy: 0.9600 - val_loss: 2.1926 - val_accuracy: 0.6800\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.9700\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2775 - accuracy: 0.9700 - val_loss: 1.0191 - val_accuracy: 0.8000\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0272 - accuracy: 0.8040\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 1.0272 - accuracy: 0.8040 - val_loss: 1.2742 - val_accuracy: 0.7400\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.8080\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.8357 - accuracy: 0.8080 - val_loss: 2.2081 - val_accuracy: 0.6600\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8620\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.5739 - accuracy: 0.8620 - val_loss: 0.8176 - val_accuracy: 0.8000\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4803 - accuracy: 0.9120\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4803 - accuracy: 0.9120 - val_loss: 0.6378 - val_accuracy: 0.8467\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.9140\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4478 - accuracy: 0.9140 - val_loss: 0.5855 - val_accuracy: 0.8600\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.9420\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3795 - accuracy: 0.9420 - val_loss: 0.5501 - val_accuracy: 0.8533\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4253 - accuracy: 0.9380\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.4253 - accuracy: 0.9380 - val_loss: 0.7009 - val_accuracy: 0.8533\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.9420\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3494 - accuracy: 0.9420 - val_loss: 0.5858 - val_accuracy: 0.8600\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.9400\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3696 - accuracy: 0.9400 - val_loss: 0.5228 - val_accuracy: 0.9000\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9640\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2825 - accuracy: 0.9640 - val_loss: 0.4690 - val_accuracy: 0.9133\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9760\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2717 - accuracy: 0.9760 - val_loss: 0.5963 - val_accuracy: 0.8333\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.9760\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2695 - accuracy: 0.9760 - val_loss: 0.6603 - val_accuracy: 0.8667\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.9400\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3309 - accuracy: 0.9400 - val_loss: 0.8162 - val_accuracy: 0.8067\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3951 - accuracy: 0.9360\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3951 - accuracy: 0.9360 - val_loss: 1.0451 - val_accuracy: 0.7533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.9660\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2922 - accuracy: 0.9660 - val_loss: 1.1075 - val_accuracy: 0.7800\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.9620\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3202 - accuracy: 0.9620 - val_loss: 1.0407 - val_accuracy: 0.8133\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3719 - accuracy: 0.9460\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3719 - accuracy: 0.9460 - val_loss: 1.0331 - val_accuracy: 0.7733\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9680\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2891 - accuracy: 0.9680 - val_loss: 0.7866 - val_accuracy: 0.8467\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2956 - accuracy: 0.9720\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2956 - accuracy: 0.9720 - val_loss: 0.8245 - val_accuracy: 0.8333\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9680\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2647 - accuracy: 0.9680 - val_loss: 0.7491 - val_accuracy: 0.8333\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9620\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2873 - accuracy: 0.9620 - val_loss: 1.3952 - val_accuracy: 0.7133\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9720\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2508 - accuracy: 0.9720 - val_loss: 1.0384 - val_accuracy: 0.8000\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9680\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2768 - accuracy: 0.9680 - val_loss: 0.9215 - val_accuracy: 0.7933\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9500\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3270 - accuracy: 0.9500 - val_loss: 1.1680 - val_accuracy: 0.8000\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9840\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2440 - accuracy: 0.9840 - val_loss: 0.7261 - val_accuracy: 0.8600\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9840\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2363 - accuracy: 0.9840 - val_loss: 1.0593 - val_accuracy: 0.8067\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9900\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2172 - accuracy: 0.9900 - val_loss: 0.8329 - val_accuracy: 0.8467\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9940\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2087 - accuracy: 0.9940 - val_loss: 0.6713 - val_accuracy: 0.8867\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2006 - accuracy: 0.9960\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2006 - accuracy: 0.9960 - val_loss: 0.6431 - val_accuracy: 0.8800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1983 - accuracy: 0.9960\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1983 - accuracy: 0.9960 - val_loss: 0.7509 - val_accuracy: 0.8600\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9960\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1926 - accuracy: 0.9960 - val_loss: 0.7010 - val_accuracy: 0.8667\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1928 - accuracy: 0.9960\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1928 - accuracy: 0.9960 - val_loss: 0.7486 - val_accuracy: 0.8600\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2123 - accuracy: 0.9900\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 294ms/step - loss: 0.2123 - accuracy: 0.9900 - val_loss: 0.7917 - val_accuracy: 0.8933\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2476 - accuracy: 0.9860\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2476 - accuracy: 0.9860 - val_loss: 0.7856 - val_accuracy: 0.8733\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9840\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2158 - accuracy: 0.9840 - val_loss: 1.0912 - val_accuracy: 0.8467\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9760\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2440 - accuracy: 0.9760 - val_loss: 0.6789 - val_accuracy: 0.8733\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9800\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2219 - accuracy: 0.9800 - val_loss: 0.9706 - val_accuracy: 0.8267\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9780\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2501 - accuracy: 0.9780 - val_loss: 0.9690 - val_accuracy: 0.8067\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9600\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2799 - accuracy: 0.9600 - val_loss: 1.1414 - val_accuracy: 0.7800\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9600\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2905 - accuracy: 0.9600 - val_loss: 0.9735 - val_accuracy: 0.8467\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.2548 - accuracy: 0.7500\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 1.2548 - accuracy: 0.7500 - val_loss: 2.0367 - val_accuracy: 0.6067\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.8260\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.7436 - accuracy: 0.8260 - val_loss: 0.8473 - val_accuracy: 0.7667\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5982 - accuracy: 0.8740\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.5982 - accuracy: 0.8740 - val_loss: 1.5991 - val_accuracy: 0.6067\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4340 - accuracy: 0.9160\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.4340 - accuracy: 0.9160 - val_loss: 0.7746 - val_accuracy: 0.8067\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3494 - accuracy: 0.9460\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3494 - accuracy: 0.9460 - val_loss: 0.4717 - val_accuracy: 0.9133\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3539 - accuracy: 0.9520\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3539 - accuracy: 0.9520 - val_loss: 0.5713 - val_accuracy: 0.8733\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.9560\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3257 - accuracy: 0.9560 - val_loss: 0.6160 - val_accuracy: 0.8733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.9540\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3254 - accuracy: 0.9540 - val_loss: 0.7298 - val_accuracy: 0.8400\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.9400\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3478 - accuracy: 0.9400 - val_loss: 0.8355 - val_accuracy: 0.8600\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.9620\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2925 - accuracy: 0.9620 - val_loss: 0.6593 - val_accuracy: 0.8600\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9640\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2864 - accuracy: 0.9640 - val_loss: 0.7119 - val_accuracy: 0.8533\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2489 - accuracy: 0.9780\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2489 - accuracy: 0.9780 - val_loss: 0.8413 - val_accuracy: 0.8333\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9880\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2236 - accuracy: 0.9880 - val_loss: 0.7621 - val_accuracy: 0.8333\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9860\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2186 - accuracy: 0.9860 - val_loss: 0.6719 - val_accuracy: 0.8533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9880\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2188 - accuracy: 0.9880 - val_loss: 0.7488 - val_accuracy: 0.8667\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9840\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2252 - accuracy: 0.9840 - val_loss: 0.7457 - val_accuracy: 0.8600\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9860\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2230 - accuracy: 0.9860 - val_loss: 0.7204 - val_accuracy: 0.8867\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.9840\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2409 - accuracy: 0.9840 - val_loss: 1.0084 - val_accuracy: 0.8133\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.9840\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2136 - accuracy: 0.9840 - val_loss: 1.2554 - val_accuracy: 0.7667\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2250 - accuracy: 0.9860\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2250 - accuracy: 0.9860 - val_loss: 1.4453 - val_accuracy: 0.7533\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9820\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2317 - accuracy: 0.9820 - val_loss: 0.9895 - val_accuracy: 0.8400\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9820\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2244 - accuracy: 0.9820 - val_loss: 1.4683 - val_accuracy: 0.7467\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.9500\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3819 - accuracy: 0.9500 - val_loss: 2.1788 - val_accuracy: 0.7333\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9540\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3340 - accuracy: 0.9540 - val_loss: 1.3376 - val_accuracy: 0.7600\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9720\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2808 - accuracy: 0.9720 - val_loss: 0.9275 - val_accuracy: 0.8200\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.9560\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3126 - accuracy: 0.9560 - val_loss: 1.3105 - val_accuracy: 0.7867\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.9760\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2562 - accuracy: 0.9760 - val_loss: 1.0006 - val_accuracy: 0.8400\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2385 - accuracy: 0.9800\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2385 - accuracy: 0.9800 - val_loss: 1.0467 - val_accuracy: 0.8000\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9860\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2140 - accuracy: 0.9860 - val_loss: 0.7918 - val_accuracy: 0.8400\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9880\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2154 - accuracy: 0.9880 - val_loss: 0.7431 - val_accuracy: 0.8800\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9860\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2267 - accuracy: 0.9860 - val_loss: 0.7878 - val_accuracy: 0.8533\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9880\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2118 - accuracy: 0.9880 - val_loss: 0.8786 - val_accuracy: 0.8267\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9880\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2204 - accuracy: 0.9880 - val_loss: 0.6687 - val_accuracy: 0.8800\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2377 - accuracy: 0.9820\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2377 - accuracy: 0.9820 - val_loss: 1.1502 - val_accuracy: 0.8200\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9900\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2035 - accuracy: 0.9900 - val_loss: 1.4419 - val_accuracy: 0.7533\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9900\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2201 - accuracy: 0.9900 - val_loss: 1.0648 - val_accuracy: 0.8267\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9800\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2221 - accuracy: 0.9800 - val_loss: 0.8766 - val_accuracy: 0.8667\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.9860\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2088 - accuracy: 0.9860 - val_loss: 0.8285 - val_accuracy: 0.8600\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9940\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1940 - accuracy: 0.9940 - val_loss: 1.2425 - val_accuracy: 0.8133\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9860\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2126 - accuracy: 0.9860 - val_loss: 0.9008 - val_accuracy: 0.8267\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.8760 - accuracy: 0.8480\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.8760 - accuracy: 0.8480 - val_loss: 1.5307 - val_accuracy: 0.6400\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7047 - accuracy: 0.8460\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.7047 - accuracy: 0.8460 - val_loss: 1.6529 - val_accuracy: 0.5733\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5250 - accuracy: 0.9020\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.5250 - accuracy: 0.9020 - val_loss: 1.1895 - val_accuracy: 0.7067\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3823 - accuracy: 0.9420\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3823 - accuracy: 0.9420 - val_loss: 0.7642 - val_accuracy: 0.8267\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9600\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3357 - accuracy: 0.9600 - val_loss: 0.6954 - val_accuracy: 0.8533\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2751 - accuracy: 0.9660\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2751 - accuracy: 0.9660 - val_loss: 0.7776 - val_accuracy: 0.8467\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.9680\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2844 - accuracy: 0.9680 - val_loss: 0.9484 - val_accuracy: 0.8133\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2799 - accuracy: 0.9580\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2799 - accuracy: 0.9580 - val_loss: 0.7743 - val_accuracy: 0.8533\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2930 - accuracy: 0.9620\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2930 - accuracy: 0.9620 - val_loss: 1.1214 - val_accuracy: 0.7667\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3172 - accuracy: 0.9520\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3172 - accuracy: 0.9520 - val_loss: 0.9531 - val_accuracy: 0.7733\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9780\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2491 - accuracy: 0.9780 - val_loss: 0.8545 - val_accuracy: 0.7933\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2849 - accuracy: 0.9660\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2849 - accuracy: 0.9660 - val_loss: 0.7901 - val_accuracy: 0.8267\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9660\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2767 - accuracy: 0.9660 - val_loss: 0.8474 - val_accuracy: 0.8133\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.9520\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3062 - accuracy: 0.9520 - val_loss: 1.8676 - val_accuracy: 0.7067\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.9480\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3386 - accuracy: 0.9480 - val_loss: 0.9801 - val_accuracy: 0.7933\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.9680\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2803 - accuracy: 0.9680 - val_loss: 0.9878 - val_accuracy: 0.7800\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.9820\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2356 - accuracy: 0.9820 - val_loss: 0.8156 - val_accuracy: 0.8400\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9860\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2193 - accuracy: 0.9860 - val_loss: 0.7128 - val_accuracy: 0.8333\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9740\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2910 - accuracy: 0.9740 - val_loss: 0.9983 - val_accuracy: 0.8067\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9760\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2529 - accuracy: 0.9760 - val_loss: 1.3088 - val_accuracy: 0.7867\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.9600\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3092 - accuracy: 0.9600 - val_loss: 1.2289 - val_accuracy: 0.6867\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3046 - accuracy: 0.9640\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3046 - accuracy: 0.9640 - val_loss: 1.2695 - val_accuracy: 0.7467\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9840\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2226 - accuracy: 0.9840 - val_loss: 1.2214 - val_accuracy: 0.8200\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2194 - accuracy: 0.9880 - val_loss: 0.9268 - val_accuracy: 0.8333\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9880\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2106 - accuracy: 0.9880 - val_loss: 0.9888 - val_accuracy: 0.7933\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9820\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2185 - accuracy: 0.9820 - val_loss: 1.4869 - val_accuracy: 0.7800\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2046 - accuracy: 0.9920 - val_loss: 1.0831 - val_accuracy: 0.7800\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2312 - accuracy: 0.9760\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2312 - accuracy: 0.9760 - val_loss: 1.5251 - val_accuracy: 0.7200\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9800\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2435 - accuracy: 0.9800 - val_loss: 1.2193 - val_accuracy: 0.7400\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.9680\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2857 - accuracy: 0.9680 - val_loss: 1.1123 - val_accuracy: 0.8000\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.9700\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3176 - accuracy: 0.9700 - val_loss: 1.6752 - val_accuracy: 0.7000\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.9720\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2877 - accuracy: 0.9720 - val_loss: 2.4194 - val_accuracy: 0.6067\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9760\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2701 - accuracy: 0.9760 - val_loss: 1.5740 - val_accuracy: 0.7467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9780\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2502 - accuracy: 0.9780 - val_loss: 1.0079 - val_accuracy: 0.7533\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2509 - accuracy: 0.9820\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2509 - accuracy: 0.9820 - val_loss: 1.0659 - val_accuracy: 0.7800\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.9720\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2579 - accuracy: 0.9720 - val_loss: 1.2723 - val_accuracy: 0.7667\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2487 - accuracy: 0.9860\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2487 - accuracy: 0.9860 - val_loss: 1.2288 - val_accuracy: 0.7533\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.9900\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2170 - accuracy: 0.9900 - val_loss: 0.9110 - val_accuracy: 0.8467\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9900\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1997 - accuracy: 0.9900 - val_loss: 1.2155 - val_accuracy: 0.8067\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9940\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1894 - accuracy: 0.9940 - val_loss: 1.3587 - val_accuracy: 0.7867\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9065 - accuracy: 0.8260\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.9065 - accuracy: 0.8260 - val_loss: 2.7641 - val_accuracy: 0.5800\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6547 - accuracy: 0.8560\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.6547 - accuracy: 0.8560 - val_loss: 1.1223 - val_accuracy: 0.7000\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.8920\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.5458 - accuracy: 0.8920 - val_loss: 0.8702 - val_accuracy: 0.8133\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.9300\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3862 - accuracy: 0.9300 - val_loss: 0.7929 - val_accuracy: 0.7867\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.9460\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3345 - accuracy: 0.9460 - val_loss: 1.0074 - val_accuracy: 0.7667\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.9480\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3440 - accuracy: 0.9480 - val_loss: 0.9285 - val_accuracy: 0.7733\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.9680\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2767 - accuracy: 0.9680 - val_loss: 0.9445 - val_accuracy: 0.7667\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.9720\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2672 - accuracy: 0.9720 - val_loss: 1.0634 - val_accuracy: 0.7733\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9800\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2526 - accuracy: 0.9800 - val_loss: 1.1035 - val_accuracy: 0.7533\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.9800\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2408 - accuracy: 0.9800 - val_loss: 1.1551 - val_accuracy: 0.7933\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3094 - accuracy: 0.9600\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3094 - accuracy: 0.9600 - val_loss: 1.2516 - val_accuracy: 0.7333\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9600\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2942 - accuracy: 0.9600 - val_loss: 1.0380 - val_accuracy: 0.8067\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9640\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2842 - accuracy: 0.9640 - val_loss: 1.3338 - val_accuracy: 0.7600\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.9600\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3224 - accuracy: 0.9600 - val_loss: 1.3574 - val_accuracy: 0.7333\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3750 - accuracy: 0.9300\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3750 - accuracy: 0.9300 - val_loss: 1.1086 - val_accuracy: 0.7600\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9620\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3201 - accuracy: 0.9620 - val_loss: 1.0711 - val_accuracy: 0.7600\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9780\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2635 - accuracy: 0.9780 - val_loss: 1.1641 - val_accuracy: 0.7400\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9820\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2273 - accuracy: 0.9820 - val_loss: 0.9830 - val_accuracy: 0.8133\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9800\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2290 - accuracy: 0.9800 - val_loss: 1.1217 - val_accuracy: 0.7800\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9860\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 305ms/step - loss: 0.2090 - accuracy: 0.9860 - val_loss: 1.0154 - val_accuracy: 0.8200\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9920\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.2094 - accuracy: 0.9920 - val_loss: 1.2019 - val_accuracy: 0.7867\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9840\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2240 - accuracy: 0.9840 - val_loss: 1.1625 - val_accuracy: 0.7867\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.9740\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2422 - accuracy: 0.9740 - val_loss: 1.1793 - val_accuracy: 0.7733\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9660\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2841 - accuracy: 0.9660 - val_loss: 1.6969 - val_accuracy: 0.7133\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9660\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2699 - accuracy: 0.9660 - val_loss: 0.9683 - val_accuracy: 0.7800\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9760\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2526 - accuracy: 0.9760 - val_loss: 1.1580 - val_accuracy: 0.8200\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9840\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2326 - accuracy: 0.9840 - val_loss: 1.0137 - val_accuracy: 0.8000\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9880\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2039 - accuracy: 0.9880 - val_loss: 1.1864 - val_accuracy: 0.7733\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2187 - accuracy: 0.9820\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2187 - accuracy: 0.9820 - val_loss: 1.5046 - val_accuracy: 0.7333\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9840\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2051 - accuracy: 0.9840 - val_loss: 1.1907 - val_accuracy: 0.8000\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9920\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1949 - accuracy: 0.9920 - val_loss: 1.1183 - val_accuracy: 0.8000\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9860\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2266 - accuracy: 0.9860 - val_loss: 2.2780 - val_accuracy: 0.6467\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9780\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2358 - accuracy: 0.9780 - val_loss: 1.2021 - val_accuracy: 0.7733\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9680\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2662 - accuracy: 0.9680 - val_loss: 1.5249 - val_accuracy: 0.7533\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9720\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2474 - accuracy: 0.9720 - val_loss: 1.6310 - val_accuracy: 0.7733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.9740\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2905 - accuracy: 0.9740 - val_loss: 1.3679 - val_accuracy: 0.7600\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9680\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2669 - accuracy: 0.9680 - val_loss: 2.2937 - val_accuracy: 0.7000\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.9760\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2388 - accuracy: 0.9760 - val_loss: 1.5552 - val_accuracy: 0.7733\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9800\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2196 - accuracy: 0.9800 - val_loss: 1.1240 - val_accuracy: 0.8067\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9900\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2142 - accuracy: 0.9900 - val_loss: 1.2398 - val_accuracy: 0.8067\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9433 - accuracy: 0.8180\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.9433 - accuracy: 0.8180 - val_loss: 1.5236 - val_accuracy: 0.7133\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7193 - accuracy: 0.8320\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.7193 - accuracy: 0.8320 - val_loss: 1.3832 - val_accuracy: 0.7333\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.8840\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.5382 - accuracy: 0.8840 - val_loss: 1.1457 - val_accuracy: 0.7600\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4384 - accuracy: 0.9220\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.4384 - accuracy: 0.9220 - val_loss: 0.8709 - val_accuracy: 0.8267\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.9420\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3855 - accuracy: 0.9420 - val_loss: 0.8614 - val_accuracy: 0.7867\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.9420\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3391 - accuracy: 0.9420 - val_loss: 0.7306 - val_accuracy: 0.8333\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.9460\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2955 - accuracy: 0.9460 - val_loss: 1.0616 - val_accuracy: 0.7800\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9600\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3098 - accuracy: 0.9600 - val_loss: 1.2343 - val_accuracy: 0.7933\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3186 - accuracy: 0.9500\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3186 - accuracy: 0.9500 - val_loss: 0.9298 - val_accuracy: 0.8133\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9800\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2635 - accuracy: 0.9800 - val_loss: 0.8141 - val_accuracy: 0.8333\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9780\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2345 - accuracy: 0.9780 - val_loss: 0.8910 - val_accuracy: 0.8600\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2349 - accuracy: 0.9800\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2349 - accuracy: 0.9800 - val_loss: 1.0523 - val_accuracy: 0.7733\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9760\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2415 - accuracy: 0.9760 - val_loss: 0.8913 - val_accuracy: 0.8400\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9780\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2271 - accuracy: 0.9780 - val_loss: 1.1700 - val_accuracy: 0.7933\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9720\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2658 - accuracy: 0.9720 - val_loss: 1.0259 - val_accuracy: 0.8467\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2910 - accuracy: 0.9660\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2910 - accuracy: 0.9660 - val_loss: 0.8656 - val_accuracy: 0.8333\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9720\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2891 - accuracy: 0.9720 - val_loss: 0.9582 - val_accuracy: 0.8200\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2480 - accuracy: 0.9700\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2480 - accuracy: 0.9700 - val_loss: 1.3410 - val_accuracy: 0.7733\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.9740\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2561 - accuracy: 0.9740 - val_loss: 0.9952 - val_accuracy: 0.8200\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.9720\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2601 - accuracy: 0.9720 - val_loss: 1.1114 - val_accuracy: 0.7800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9760\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2699 - accuracy: 0.9760 - val_loss: 0.9967 - val_accuracy: 0.7600\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9700\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2332 - accuracy: 0.9700 - val_loss: 1.1148 - val_accuracy: 0.8133\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3374 - accuracy: 0.9540\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3374 - accuracy: 0.9540 - val_loss: 1.4836 - val_accuracy: 0.7267\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3050 - accuracy: 0.9680\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.3050 - accuracy: 0.9680 - val_loss: 1.2972 - val_accuracy: 0.7667\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9800\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2534 - accuracy: 0.9800 - val_loss: 0.9874 - val_accuracy: 0.8067\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2144 - accuracy: 0.9820\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2144 - accuracy: 0.9820 - val_loss: 1.0271 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9960\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1849 - accuracy: 0.9960 - val_loss: 0.7742 - val_accuracy: 0.8400\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9920\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2013 - accuracy: 0.9920 - val_loss: 1.1722 - val_accuracy: 0.8067\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9980\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1816 - accuracy: 0.9980 - val_loss: 1.1440 - val_accuracy: 0.8000\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1813 - accuracy: 0.9940\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 0.1813 - accuracy: 0.9940 - val_loss: 1.0246 - val_accuracy: 0.8267\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9840\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2054 - accuracy: 0.9840 - val_loss: 1.0526 - val_accuracy: 0.8267\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9880\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1924 - accuracy: 0.9880 - val_loss: 1.1197 - val_accuracy: 0.8000\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9960\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1786 - accuracy: 0.9960 - val_loss: 1.0538 - val_accuracy: 0.8133\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9880\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1940 - accuracy: 0.9880 - val_loss: 0.9513 - val_accuracy: 0.8267\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2146 - accuracy: 0.9860\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2146 - accuracy: 0.9860 - val_loss: 1.1206 - val_accuracy: 0.7733\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9700\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2879 - accuracy: 0.9700 - val_loss: 1.3106 - val_accuracy: 0.8133\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9720\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2259 - accuracy: 0.9720 - val_loss: 1.7228 - val_accuracy: 0.7000\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.9640\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3365 - accuracy: 0.9640 - val_loss: 1.1424 - val_accuracy: 0.8067\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3810 - accuracy: 0.9300\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3810 - accuracy: 0.9300 - val_loss: 1.1637 - val_accuracy: 0.8267\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.9460\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3917 - accuracy: 0.9460 - val_loss: 2.8807 - val_accuracy: 0.4867\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9619 - accuracy: 0.8200\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 0.9619 - accuracy: 0.8200 - val_loss: 1.3989 - val_accuracy: 0.6533\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7844 - accuracy: 0.8040\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.7844 - accuracy: 0.8040 - val_loss: 0.9334 - val_accuracy: 0.7400\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5147 - accuracy: 0.8820\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.5147 - accuracy: 0.8820 - val_loss: 0.6923 - val_accuracy: 0.8333\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.9200\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4136 - accuracy: 0.9200 - val_loss: 0.8210 - val_accuracy: 0.7867\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.9480\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3389 - accuracy: 0.9480 - val_loss: 0.6634 - val_accuracy: 0.8533\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.9660\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3008 - accuracy: 0.9660 - val_loss: 0.6975 - val_accuracy: 0.8533\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2720 - accuracy: 0.9760\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 293ms/step - loss: 0.2720 - accuracy: 0.9760 - val_loss: 0.7887 - val_accuracy: 0.8467\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9760\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2592 - accuracy: 0.9760 - val_loss: 0.7279 - val_accuracy: 0.8600\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9740\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2474 - accuracy: 0.9740 - val_loss: 0.8075 - val_accuracy: 0.8533\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9800\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2254 - accuracy: 0.9800 - val_loss: 0.7960 - val_accuracy: 0.8533\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2295 - accuracy: 0.9780\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2295 - accuracy: 0.9780 - val_loss: 1.1205 - val_accuracy: 0.7733\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9740\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2503 - accuracy: 0.9740 - val_loss: 0.9907 - val_accuracy: 0.8133\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9680\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2604 - accuracy: 0.9680 - val_loss: 0.7780 - val_accuracy: 0.8467\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9780\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2586 - accuracy: 0.9780 - val_loss: 0.6295 - val_accuracy: 0.8400\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3168 - accuracy: 0.9560\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3168 - accuracy: 0.9560 - val_loss: 0.9055 - val_accuracy: 0.8533\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.9460\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3396 - accuracy: 0.9460 - val_loss: 1.0318 - val_accuracy: 0.7800\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.9360\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3808 - accuracy: 0.9360 - val_loss: 1.0504 - val_accuracy: 0.8133\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3607 - accuracy: 0.9460\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3607 - accuracy: 0.9460 - val_loss: 0.7849 - val_accuracy: 0.8333\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.9740\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2800 - accuracy: 0.9740 - val_loss: 0.7626 - val_accuracy: 0.8267\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9780\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2431 - accuracy: 0.9780 - val_loss: 0.8050 - val_accuracy: 0.8467\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9920\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2021 - accuracy: 0.9920 - val_loss: 0.6947 - val_accuracy: 0.8733\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9800\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2172 - accuracy: 0.9800 - val_loss: 0.6879 - val_accuracy: 0.8667\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9740\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2545 - accuracy: 0.9740 - val_loss: 0.7430 - val_accuracy: 0.8867\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9800\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2148 - accuracy: 0.9800 - val_loss: 0.8356 - val_accuracy: 0.8467\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2240 - accuracy: 0.9900\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2240 - accuracy: 0.9900 - val_loss: 0.7886 - val_accuracy: 0.8733\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9820\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2528 - accuracy: 0.9820 - val_loss: 0.8115 - val_accuracy: 0.8533\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9860\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2362 - accuracy: 0.9860 - val_loss: 0.7986 - val_accuracy: 0.8667\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9940\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2030 - accuracy: 0.9940 - val_loss: 0.9463 - val_accuracy: 0.8533\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2359 - accuracy: 0.9780\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2359 - accuracy: 0.9780 - val_loss: 0.8819 - val_accuracy: 0.8800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9880\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2014 - accuracy: 0.9880 - val_loss: 0.9845 - val_accuracy: 0.8400\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9940\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1896 - accuracy: 0.9940 - val_loss: 0.7999 - val_accuracy: 0.8600\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9960\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1815 - accuracy: 0.9960 - val_loss: 0.8298 - val_accuracy: 0.8667\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1988 - accuracy: 0.9900\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1988 - accuracy: 0.9900 - val_loss: 0.7522 - val_accuracy: 0.8800\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1976 - accuracy: 0.9900\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1976 - accuracy: 0.9900 - val_loss: 0.8765 - val_accuracy: 0.8400\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9980\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1696 - accuracy: 0.9980 - val_loss: 0.9448 - val_accuracy: 0.8467\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1951 - accuracy: 0.9880\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1951 - accuracy: 0.9880 - val_loss: 0.9788 - val_accuracy: 0.8467\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9900\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1860 - accuracy: 0.9900 - val_loss: 0.9846 - val_accuracy: 0.8600\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1861 - accuracy: 0.9920\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1861 - accuracy: 0.9920 - val_loss: 0.9709 - val_accuracy: 0.8267\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1968 - accuracy: 0.9900\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1968 - accuracy: 0.9900 - val_loss: 1.2876 - val_accuracy: 0.7867\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9860\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2012 - accuracy: 0.9860 - val_loss: 1.2188 - val_accuracy: 0.7800\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1755 - accuracy: 0.7620\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 297ms/step - loss: 1.1755 - accuracy: 0.7620 - val_loss: 2.0738 - val_accuracy: 0.5933\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.8280\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.6737 - accuracy: 0.8280 - val_loss: 0.9959 - val_accuracy: 0.7467\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.8960\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4885 - accuracy: 0.8960 - val_loss: 0.7826 - val_accuracy: 0.8600\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.9160\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3939 - accuracy: 0.9160 - val_loss: 0.7572 - val_accuracy: 0.8333\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3606 - accuracy: 0.9440\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3606 - accuracy: 0.9440 - val_loss: 0.6063 - val_accuracy: 0.8600\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.9600\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3026 - accuracy: 0.9600 - val_loss: 0.6152 - val_accuracy: 0.8533\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9640\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2702 - accuracy: 0.9640 - val_loss: 0.7214 - val_accuracy: 0.8267\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2632 - accuracy: 0.9760\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2632 - accuracy: 0.9760 - val_loss: 1.1755 - val_accuracy: 0.7800\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.9520\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3130 - accuracy: 0.9520 - val_loss: 0.7157 - val_accuracy: 0.8267\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9700\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2609 - accuracy: 0.9700 - val_loss: 0.9096 - val_accuracy: 0.8267\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9700\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2495 - accuracy: 0.9700 - val_loss: 0.8206 - val_accuracy: 0.8533\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2501 - accuracy: 0.9720\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2501 - accuracy: 0.9720 - val_loss: 1.1056 - val_accuracy: 0.7867\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.9520\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3308 - accuracy: 0.9520 - val_loss: 0.8463 - val_accuracy: 0.8067\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.9360\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3311 - accuracy: 0.9360 - val_loss: 0.7813 - val_accuracy: 0.8533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.9540\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3077 - accuracy: 0.9540 - val_loss: 0.9805 - val_accuracy: 0.7733\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9440\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3073 - accuracy: 0.9440 - val_loss: 1.0405 - val_accuracy: 0.7933\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9560\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3098 - accuracy: 0.9560 - val_loss: 1.0922 - val_accuracy: 0.7733\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.9580\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2926 - accuracy: 0.9580 - val_loss: 0.7345 - val_accuracy: 0.8733\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.9740\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2366 - accuracy: 0.9740 - val_loss: 0.7379 - val_accuracy: 0.8733\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2014 - accuracy: 0.9900\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2014 - accuracy: 0.9900 - val_loss: 0.7262 - val_accuracy: 0.8533\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9880\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1958 - accuracy: 0.9880 - val_loss: 0.9216 - val_accuracy: 0.8333\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9900\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2110 - accuracy: 0.9900 - val_loss: 0.9107 - val_accuracy: 0.8200\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2502 - accuracy: 0.9720\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2502 - accuracy: 0.9720 - val_loss: 0.7998 - val_accuracy: 0.8467\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2637 - accuracy: 0.9680\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2637 - accuracy: 0.9680 - val_loss: 0.7296 - val_accuracy: 0.8600\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2302 - accuracy: 0.9760\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2302 - accuracy: 0.9760 - val_loss: 0.7957 - val_accuracy: 0.8400\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9820\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2244 - accuracy: 0.9820 - val_loss: 0.7456 - val_accuracy: 0.8667\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9780\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2403 - accuracy: 0.9780 - val_loss: 0.7769 - val_accuracy: 0.8600\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2985 - accuracy: 0.9560\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2985 - accuracy: 0.9560 - val_loss: 1.3382 - val_accuracy: 0.8000\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9620\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2710 - accuracy: 0.9620 - val_loss: 1.2438 - val_accuracy: 0.8000\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2293 - accuracy: 0.9760\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2293 - accuracy: 0.9760 - val_loss: 1.0040 - val_accuracy: 0.7933\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9860\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2210 - accuracy: 0.9860 - val_loss: 0.8397 - val_accuracy: 0.8867\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2307 - accuracy: 0.9840\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2307 - accuracy: 0.9840 - val_loss: 0.9115 - val_accuracy: 0.8533\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2164 - accuracy: 0.9840\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2164 - accuracy: 0.9840 - val_loss: 0.9252 - val_accuracy: 0.8200\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9920\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1896 - accuracy: 0.9920 - val_loss: 0.9351 - val_accuracy: 0.8333\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9940\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1849 - accuracy: 0.9940 - val_loss: 0.9523 - val_accuracy: 0.8400\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1967 - accuracy: 0.9940\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1967 - accuracy: 0.9940 - val_loss: 1.0289 - val_accuracy: 0.8333\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9960\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1755 - accuracy: 0.9960 - val_loss: 1.0918 - val_accuracy: 0.8200\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9820\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2344 - accuracy: 0.9820 - val_loss: 1.0724 - val_accuracy: 0.7933\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2276 - accuracy: 0.9820\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2276 - accuracy: 0.9820 - val_loss: 1.0544 - val_accuracy: 0.7933\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9940\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1890 - accuracy: 0.9940 - val_loss: 1.2463 - val_accuracy: 0.8000\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7825 - accuracy: 0.8480\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.7825 - accuracy: 0.8480 - val_loss: 1.1592 - val_accuracy: 0.8067\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5794 - accuracy: 0.8720\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.5794 - accuracy: 0.8720 - val_loss: 1.0909 - val_accuracy: 0.7533\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4113 - accuracy: 0.9300\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.4113 - accuracy: 0.9300 - val_loss: 0.8630 - val_accuracy: 0.8067\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.9360\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3414 - accuracy: 0.9360 - val_loss: 0.6204 - val_accuracy: 0.8600\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2946 - accuracy: 0.9640\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2946 - accuracy: 0.9640 - val_loss: 0.5706 - val_accuracy: 0.9000\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9740\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2635 - accuracy: 0.9740 - val_loss: 0.7581 - val_accuracy: 0.8600\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2205 - accuracy: 0.9840\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2205 - accuracy: 0.9840 - val_loss: 0.8455 - val_accuracy: 0.8733\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9880\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2108 - accuracy: 0.9880 - val_loss: 0.6817 - val_accuracy: 0.8733\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.9900\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2108 - accuracy: 0.9900 - val_loss: 0.7831 - val_accuracy: 0.8733\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.9880\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2082 - accuracy: 0.9880 - val_loss: 0.7152 - val_accuracy: 0.9000\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9880\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2012 - accuracy: 0.9880 - val_loss: 0.9573 - val_accuracy: 0.8400\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9920\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1896 - accuracy: 0.9920 - val_loss: 0.9456 - val_accuracy: 0.8800\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9840\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2165 - accuracy: 0.9840 - val_loss: 0.7912 - val_accuracy: 0.8800\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 0.9820\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1999 - accuracy: 0.9820 - val_loss: 0.6226 - val_accuracy: 0.9000\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2235 - accuracy: 0.9820\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2235 - accuracy: 0.9820 - val_loss: 0.9193 - val_accuracy: 0.8600\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9920\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1941 - accuracy: 0.9920 - val_loss: 1.1379 - val_accuracy: 0.8200\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9860\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2089 - accuracy: 0.9860 - val_loss: 0.8819 - val_accuracy: 0.8600\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2422 - accuracy: 0.9720\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2422 - accuracy: 0.9720 - val_loss: 0.8947 - val_accuracy: 0.8400\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9720\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2449 - accuracy: 0.9720 - val_loss: 0.7083 - val_accuracy: 0.8600\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.9620\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3288 - accuracy: 0.9620 - val_loss: 1.6164 - val_accuracy: 0.7333\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3541 - accuracy: 0.9320\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3541 - accuracy: 0.9320 - val_loss: 1.4196 - val_accuracy: 0.7933\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.9600\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2848 - accuracy: 0.9600 - val_loss: 1.1632 - val_accuracy: 0.7467\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2586 - accuracy: 0.9760\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2586 - accuracy: 0.9760 - val_loss: 1.3015 - val_accuracy: 0.7067\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9860\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2080 - accuracy: 0.9860 - val_loss: 0.8242 - val_accuracy: 0.8267\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9860\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2141 - accuracy: 0.9860 - val_loss: 0.7907 - val_accuracy: 0.8400\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2390 - accuracy: 0.9800\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2390 - accuracy: 0.9800 - val_loss: 0.7810 - val_accuracy: 0.8933\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9820\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2196 - accuracy: 0.9820 - val_loss: 0.8660 - val_accuracy: 0.8600\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9860\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1998 - accuracy: 0.9860 - val_loss: 1.0422 - val_accuracy: 0.8467\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9900\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2004 - accuracy: 0.9900 - val_loss: 1.3485 - val_accuracy: 0.7933\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9980\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1786 - accuracy: 0.9980 - val_loss: 0.9718 - val_accuracy: 0.8400\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9920\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1848 - accuracy: 0.9920 - val_loss: 0.9080 - val_accuracy: 0.8867\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9940\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1704 - accuracy: 0.9940 - val_loss: 0.7542 - val_accuracy: 0.8733\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 1.0000\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1630 - accuracy: 1.0000 - val_loss: 0.7816 - val_accuracy: 0.8600\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9980\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1634 - accuracy: 0.9980 - val_loss: 1.0665 - val_accuracy: 0.8400\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9960\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1676 - accuracy: 0.9960 - val_loss: 0.9610 - val_accuracy: 0.8667\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1702 - accuracy: 0.9960\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1702 - accuracy: 0.9960 - val_loss: 1.2278 - val_accuracy: 0.8333\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9960\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1657 - accuracy: 0.9960 - val_loss: 1.7681 - val_accuracy: 0.8133\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9900\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1666 - accuracy: 0.9900 - val_loss: 0.8873 - val_accuracy: 0.8800\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9940\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1677 - accuracy: 0.9940 - val_loss: 1.0835 - val_accuracy: 0.8333\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9960\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1654 - accuracy: 0.9960 - val_loss: 1.3725 - val_accuracy: 0.8200\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.2781 - accuracy: 0.7900\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 1.2781 - accuracy: 0.7900 - val_loss: 0.5869 - val_accuracy: 0.8667\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.8360\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.6392 - accuracy: 0.8360 - val_loss: 0.9879 - val_accuracy: 0.6867\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.8820\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.4844 - accuracy: 0.8820 - val_loss: 0.5579 - val_accuracy: 0.9133\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.9360\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3415 - accuracy: 0.9360 - val_loss: 0.5635 - val_accuracy: 0.8867\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9460\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3209 - accuracy: 0.9460 - val_loss: 0.6145 - val_accuracy: 0.8800\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3075 - accuracy: 0.9520\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3075 - accuracy: 0.9520 - val_loss: 0.4991 - val_accuracy: 0.9000\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2924 - accuracy: 0.9700\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2924 - accuracy: 0.9700 - val_loss: 0.4762 - val_accuracy: 0.9200\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2699 - accuracy: 0.9580\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2699 - accuracy: 0.9580 - val_loss: 0.9709 - val_accuracy: 0.8133\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2225 - accuracy: 0.9780\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2225 - accuracy: 0.9780 - val_loss: 0.6816 - val_accuracy: 0.8667\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9920\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1965 - accuracy: 0.9920 - val_loss: 0.5167 - val_accuracy: 0.9067\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2114 - accuracy: 0.9820\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2114 - accuracy: 0.9820 - val_loss: 0.5758 - val_accuracy: 0.9000\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9880\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2031 - accuracy: 0.9880 - val_loss: 0.6125 - val_accuracy: 0.8800\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.9640\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2691 - accuracy: 0.9640 - val_loss: 0.7286 - val_accuracy: 0.9067\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9700\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2635 - accuracy: 0.9700 - val_loss: 1.2169 - val_accuracy: 0.8000\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.9820\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2363 - accuracy: 0.9820 - val_loss: 1.3699 - val_accuracy: 0.7400\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9780\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2397 - accuracy: 0.9780 - val_loss: 0.8855 - val_accuracy: 0.8267\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9800\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2290 - accuracy: 0.9800 - val_loss: 0.9899 - val_accuracy: 0.8400\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.9720\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2311 - accuracy: 0.9720 - val_loss: 0.7158 - val_accuracy: 0.8533\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9780\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2230 - accuracy: 0.9780 - val_loss: 0.6675 - val_accuracy: 0.8933\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1783 - accuracy: 0.9940\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1783 - accuracy: 0.9940 - val_loss: 0.5966 - val_accuracy: 0.8933\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2200 - accuracy: 0.9820\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2200 - accuracy: 0.9820 - val_loss: 1.0125 - val_accuracy: 0.7933\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9880\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1916 - accuracy: 0.9880 - val_loss: 0.8812 - val_accuracy: 0.8600\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9860\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1833 - accuracy: 0.9860 - val_loss: 0.7206 - val_accuracy: 0.8733\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9840\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2045 - accuracy: 0.9840 - val_loss: 0.7235 - val_accuracy: 0.8667\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9900\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1840 - accuracy: 0.9900 - val_loss: 1.1466 - val_accuracy: 0.7733\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9900\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1840 - accuracy: 0.9900 - val_loss: 0.9940 - val_accuracy: 0.8267\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.9760\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2286 - accuracy: 0.9760 - val_loss: 0.9659 - val_accuracy: 0.8533\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2051 - accuracy: 0.9780\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2051 - accuracy: 0.9780 - val_loss: 0.8750 - val_accuracy: 0.8667\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9880\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1825 - accuracy: 0.9880 - val_loss: 0.8450 - val_accuracy: 0.8800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9980\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1659 - accuracy: 0.9980 - val_loss: 0.9035 - val_accuracy: 0.8400\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1615 - accuracy: 0.9960\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1615 - accuracy: 0.9960 - val_loss: 0.9220 - val_accuracy: 0.8400\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9980\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1566 - accuracy: 0.9980 - val_loss: 0.7417 - val_accuracy: 0.8800\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 1.0000\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1519 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.8933\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9980\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1550 - accuracy: 0.9980 - val_loss: 0.8831 - val_accuracy: 0.8600\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9960\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1559 - accuracy: 0.9960 - val_loss: 0.9488 - val_accuracy: 0.8667\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9960\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1535 - accuracy: 0.9960 - val_loss: 0.7979 - val_accuracy: 0.8800\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9900\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1840 - accuracy: 0.9900 - val_loss: 0.7607 - val_accuracy: 0.9067\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9760\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2090 - accuracy: 0.9760 - val_loss: 1.1113 - val_accuracy: 0.8333\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9760\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2270 - accuracy: 0.9760 - val_loss: 1.3576 - val_accuracy: 0.7867\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1816 - accuracy: 0.9920\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1816 - accuracy: 0.9920 - val_loss: 0.8699 - val_accuracy: 0.8733\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0089 - accuracy: 0.7880\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 1.0089 - accuracy: 0.7880 - val_loss: 1.9579 - val_accuracy: 0.5733\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6411 - accuracy: 0.8420\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.6411 - accuracy: 0.8420 - val_loss: 1.1237 - val_accuracy: 0.7467\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5571 - accuracy: 0.8620\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.5571 - accuracy: 0.8620 - val_loss: 0.9955 - val_accuracy: 0.7333\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4020 - accuracy: 0.9220\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4020 - accuracy: 0.9220 - val_loss: 1.0965 - val_accuracy: 0.7467\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3110 - accuracy: 0.9560\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3110 - accuracy: 0.9560 - val_loss: 0.7797 - val_accuracy: 0.7867\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9580\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2957 - accuracy: 0.9580 - val_loss: 0.9314 - val_accuracy: 0.7933\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2444 - accuracy: 0.9800\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2444 - accuracy: 0.9800 - val_loss: 0.8043 - val_accuracy: 0.7867\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2532 - accuracy: 0.9660\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2532 - accuracy: 0.9660 - val_loss: 0.9260 - val_accuracy: 0.8267\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.9520\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3119 - accuracy: 0.9520 - val_loss: 0.6889 - val_accuracy: 0.8467\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.9480\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3228 - accuracy: 0.9480 - val_loss: 0.9933 - val_accuracy: 0.7667\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.9580\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3411 - accuracy: 0.9580 - val_loss: 1.3300 - val_accuracy: 0.7267\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2895 - accuracy: 0.9600\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2895 - accuracy: 0.9600 - val_loss: 0.9194 - val_accuracy: 0.8200\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9820\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2150 - accuracy: 0.9820 - val_loss: 0.8643 - val_accuracy: 0.7667\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9860\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2055 - accuracy: 0.9860 - val_loss: 1.4214 - val_accuracy: 0.7667\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9740\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2201 - accuracy: 0.9740 - val_loss: 0.9634 - val_accuracy: 0.8133\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9860\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1949 - accuracy: 0.9860 - val_loss: 0.7153 - val_accuracy: 0.8400\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9840\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2415 - accuracy: 0.9840 - val_loss: 0.8755 - val_accuracy: 0.7800\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2543 - accuracy: 0.9640\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2543 - accuracy: 0.9640 - val_loss: 0.8377 - val_accuracy: 0.8133\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9520\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.3201 - accuracy: 0.9520 - val_loss: 1.4348 - val_accuracy: 0.7067\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.9540\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3454 - accuracy: 0.9540 - val_loss: 2.4514 - val_accuracy: 0.5800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.9500\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3071 - accuracy: 0.9500 - val_loss: 1.6337 - val_accuracy: 0.7267\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9680\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2483 - accuracy: 0.9680 - val_loss: 1.0804 - val_accuracy: 0.7867\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9740\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2335 - accuracy: 0.9740 - val_loss: 1.3058 - val_accuracy: 0.7600\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1912 - accuracy: 0.9880 - val_loss: 1.5163 - val_accuracy: 0.7400\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9780\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2149 - accuracy: 0.9780 - val_loss: 0.9331 - val_accuracy: 0.8067\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9980\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1771 - accuracy: 0.9980 - val_loss: 0.7336 - val_accuracy: 0.8600\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1860 - accuracy: 0.9920 - val_loss: 1.0119 - val_accuracy: 0.8067\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2071 - accuracy: 0.9860\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2071 - accuracy: 0.9860 - val_loss: 0.9867 - val_accuracy: 0.8200\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2048 - accuracy: 0.9880\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2048 - accuracy: 0.9880 - val_loss: 0.8005 - val_accuracy: 0.8200\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9900\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1998 - accuracy: 0.9900 - val_loss: 1.0391 - val_accuracy: 0.7667\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9800\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2251 - accuracy: 0.9800 - val_loss: 0.8519 - val_accuracy: 0.8333\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9900\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1835 - accuracy: 0.9900 - val_loss: 0.9243 - val_accuracy: 0.8400\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9940\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1711 - accuracy: 0.9940 - val_loss: 1.9839 - val_accuracy: 0.7200\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1732 - accuracy: 0.9920\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1732 - accuracy: 0.9920 - val_loss: 1.4688 - val_accuracy: 0.7733\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9840\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1930 - accuracy: 0.9840 - val_loss: 0.9813 - val_accuracy: 0.8333\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9900\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1892 - accuracy: 0.9900 - val_loss: 1.2061 - val_accuracy: 0.8400\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9900\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.1800 - accuracy: 0.9900 - val_loss: 1.1430 - val_accuracy: 0.8467\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1669 - accuracy: 0.9940\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1669 - accuracy: 0.9940 - val_loss: 0.9449 - val_accuracy: 0.8400\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 1.0000\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1591 - accuracy: 1.0000 - val_loss: 1.2071 - val_accuracy: 0.8133\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9960\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1602 - accuracy: 0.9960 - val_loss: 1.2381 - val_accuracy: 0.8067\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0920 - accuracy: 0.8180\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 295ms/step - loss: 1.0920 - accuracy: 0.8180 - val_loss: 1.4407 - val_accuracy: 0.6467\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8540\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.5862 - accuracy: 0.8540 - val_loss: 0.7744 - val_accuracy: 0.8333\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.9020\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.4608 - accuracy: 0.9020 - val_loss: 0.8308 - val_accuracy: 0.7600\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.9380\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3479 - accuracy: 0.9380 - val_loss: 0.7430 - val_accuracy: 0.8267\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.9560\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2929 - accuracy: 0.9560 - val_loss: 0.6715 - val_accuracy: 0.8600\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3092 - accuracy: 0.9480\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3092 - accuracy: 0.9480 - val_loss: 0.5624 - val_accuracy: 0.8667\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.9740\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2712 - accuracy: 0.9740 - val_loss: 0.8338 - val_accuracy: 0.8467\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3419 - accuracy: 0.9440\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3419 - accuracy: 0.9440 - val_loss: 0.8501 - val_accuracy: 0.8067\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.9780\n",
            "Epoch 9: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2625 - accuracy: 0.9780 - val_loss: 0.6576 - val_accuracy: 0.8333\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9740\n",
            "Epoch 10: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2253 - accuracy: 0.9740 - val_loss: 0.6448 - val_accuracy: 0.8733\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9840\n",
            "Epoch 11: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2095 - accuracy: 0.9840 - val_loss: 0.6098 - val_accuracy: 0.8600\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9840\n",
            "Epoch 12: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2156 - accuracy: 0.9840 - val_loss: 0.8630 - val_accuracy: 0.8533\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9740\n",
            "Epoch 13: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2259 - accuracy: 0.9740 - val_loss: 0.9570 - val_accuracy: 0.8333\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9740\n",
            "Epoch 14: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2313 - accuracy: 0.9740 - val_loss: 0.6804 - val_accuracy: 0.8600\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2067 - accuracy: 0.9840\n",
            "Epoch 15: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2067 - accuracy: 0.9840 - val_loss: 0.7145 - val_accuracy: 0.8733\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9900\n",
            "Epoch 16: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2022 - accuracy: 0.9900 - val_loss: 0.8176 - val_accuracy: 0.8333\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9980\n",
            "Epoch 17: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 303ms/step - loss: 0.1677 - accuracy: 0.9980 - val_loss: 0.7212 - val_accuracy: 0.8400\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9920\n",
            "Epoch 18: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1800 - accuracy: 0.9920 - val_loss: 0.6971 - val_accuracy: 0.8467\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9900\n",
            "Epoch 19: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1804 - accuracy: 0.9900 - val_loss: 0.8482 - val_accuracy: 0.8533\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9920\n",
            "Epoch 20: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1796 - accuracy: 0.9920 - val_loss: 0.8637 - val_accuracy: 0.8333\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9820\n",
            "Epoch 21: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1997 - accuracy: 0.9820 - val_loss: 1.1786 - val_accuracy: 0.8333\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9920\n",
            "Epoch 22: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1782 - accuracy: 0.9920 - val_loss: 0.8197 - val_accuracy: 0.8800\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 0.9840\n",
            "Epoch 23: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1927 - accuracy: 0.9840 - val_loss: 1.0367 - val_accuracy: 0.8200\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9900\n",
            "Epoch 24: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1723 - accuracy: 0.9900 - val_loss: 0.9054 - val_accuracy: 0.8200\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9740\n",
            "Epoch 25: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2201 - accuracy: 0.9740 - val_loss: 1.1856 - val_accuracy: 0.8333\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.9560\n",
            "Epoch 26: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2816 - accuracy: 0.9560 - val_loss: 1.5300 - val_accuracy: 0.7800\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.9500\n",
            "Epoch 27: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3105 - accuracy: 0.9500 - val_loss: 1.5955 - val_accuracy: 0.7400\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.9500\n",
            "Epoch 28: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3039 - accuracy: 0.9500 - val_loss: 1.2434 - val_accuracy: 0.8267\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.9720\n",
            "Epoch 29: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2530 - accuracy: 0.9720 - val_loss: 1.0984 - val_accuracy: 0.8067\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2066 - accuracy: 0.9780\n",
            "Epoch 30: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2066 - accuracy: 0.9780 - val_loss: 0.9810 - val_accuracy: 0.8200\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2284 - accuracy: 0.9840\n",
            "Epoch 31: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2284 - accuracy: 0.9840 - val_loss: 1.3021 - val_accuracy: 0.7933\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9800\n",
            "Epoch 32: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2185 - accuracy: 0.9800 - val_loss: 1.1268 - val_accuracy: 0.8467\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9880\n",
            "Epoch 33: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2039 - accuracy: 0.9880 - val_loss: 0.9056 - val_accuracy: 0.8400\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9980\n",
            "Epoch 34: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1721 - accuracy: 0.9980 - val_loss: 0.9981 - val_accuracy: 0.8333\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9920\n",
            "Epoch 35: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1717 - accuracy: 0.9920 - val_loss: 1.1228 - val_accuracy: 0.8133\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1819 - accuracy: 0.9920\n",
            "Epoch 36: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1819 - accuracy: 0.9920 - val_loss: 0.7759 - val_accuracy: 0.8533\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9840\n",
            "Epoch 37: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2142 - accuracy: 0.9840 - val_loss: 0.9116 - val_accuracy: 0.8800\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9860\n",
            "Epoch 38: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2109 - accuracy: 0.9860 - val_loss: 1.1935 - val_accuracy: 0.7667\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9940\n",
            "Epoch 39: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1721 - accuracy: 0.9940 - val_loss: 0.8745 - val_accuracy: 0.8667\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9960\n",
            "Epoch 40: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1672 - accuracy: 0.9960 - val_loss: 0.8104 - val_accuracy: 0.8667\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0245 - accuracy: 0.8360\n",
            "Epoch 1: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 1.0245 - accuracy: 0.8360 - val_loss: 1.5274 - val_accuracy: 0.6933\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.8440\n",
            "Epoch 2: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.6111 - accuracy: 0.8440 - val_loss: 0.7373 - val_accuracy: 0.8267\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4549 - accuracy: 0.9020\n",
            "Epoch 3: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.4549 - accuracy: 0.9020 - val_loss: 0.5898 - val_accuracy: 0.8733\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.9260\n",
            "Epoch 4: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.3853 - accuracy: 0.9260 - val_loss: 0.6572 - val_accuracy: 0.8600\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.9300\n",
            "Epoch 5: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3792 - accuracy: 0.9300 - val_loss: 0.5927 - val_accuracy: 0.8733\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2918 - accuracy: 0.9480\n",
            "Epoch 6: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2918 - accuracy: 0.9480 - val_loss: 0.6412 - val_accuracy: 0.8467\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9700\n",
            "Epoch 7: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2513 - accuracy: 0.9700 - val_loss: 0.6701 - val_accuracy: 0.8533\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9720\n",
            "Epoch 8: val_accuracy did not improve from 0.92000\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2479 - accuracy: 0.9720 - val_loss: 0.5011 - val_accuracy: 0.9000\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9780\n",
            "Epoch 9: val_accuracy improved from 0.92000 to 0.92667, saving model to drive/MyDrive/dev/prak_nn_1/best.h5\n",
            "16/16 [==============================] - 7s 460ms/step - loss: 0.2189 - accuracy: 0.9780 - val_loss: 0.4782 - val_accuracy: 0.9267\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2561 - accuracy: 0.9700\n",
            "Epoch 10: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2561 - accuracy: 0.9700 - val_loss: 0.4877 - val_accuracy: 0.8933\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9740\n",
            "Epoch 11: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2569 - accuracy: 0.9740 - val_loss: 0.6787 - val_accuracy: 0.8667\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2605 - accuracy: 0.9700\n",
            "Epoch 12: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2605 - accuracy: 0.9700 - val_loss: 0.8129 - val_accuracy: 0.8400\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9580\n",
            "Epoch 13: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2876 - accuracy: 0.9580 - val_loss: 0.6017 - val_accuracy: 0.8667\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2474 - accuracy: 0.9720\n",
            "Epoch 14: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2474 - accuracy: 0.9720 - val_loss: 0.6404 - val_accuracy: 0.9067\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9860\n",
            "Epoch 15: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2032 - accuracy: 0.9860 - val_loss: 0.8056 - val_accuracy: 0.8267\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9800\n",
            "Epoch 16: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2179 - accuracy: 0.9800 - val_loss: 0.6490 - val_accuracy: 0.8800\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2044 - accuracy: 0.9820\n",
            "Epoch 17: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2044 - accuracy: 0.9820 - val_loss: 0.9887 - val_accuracy: 0.7933\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2141 - accuracy: 0.9860\n",
            "Epoch 18: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2141 - accuracy: 0.9860 - val_loss: 0.6078 - val_accuracy: 0.8933\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9840\n",
            "Epoch 19: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2053 - accuracy: 0.9840 - val_loss: 0.8552 - val_accuracy: 0.8933\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9840\n",
            "Epoch 20: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2304 - accuracy: 0.9840 - val_loss: 0.8771 - val_accuracy: 0.8133\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.9560\n",
            "Epoch 21: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2957 - accuracy: 0.9560 - val_loss: 1.3365 - val_accuracy: 0.7267\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.9500\n",
            "Epoch 22: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2953 - accuracy: 0.9500 - val_loss: 0.5680 - val_accuracy: 0.8800\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2325 - accuracy: 0.9800\n",
            "Epoch 23: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2325 - accuracy: 0.9800 - val_loss: 0.7363 - val_accuracy: 0.8733\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2170 - accuracy: 0.9760\n",
            "Epoch 24: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2170 - accuracy: 0.9760 - val_loss: 0.6793 - val_accuracy: 0.8600\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9860\n",
            "Epoch 25: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1997 - accuracy: 0.9860 - val_loss: 0.8801 - val_accuracy: 0.8467\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9920\n",
            "Epoch 26: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1930 - accuracy: 0.9920 - val_loss: 0.9986 - val_accuracy: 0.8333\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9940\n",
            "Epoch 27: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1769 - accuracy: 0.9940 - val_loss: 0.8086 - val_accuracy: 0.8333\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9980\n",
            "Epoch 28: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1723 - accuracy: 0.9980 - val_loss: 0.6086 - val_accuracy: 0.9000\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1784 - accuracy: 0.9880\n",
            "Epoch 29: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1784 - accuracy: 0.9880 - val_loss: 0.6327 - val_accuracy: 0.8800\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9820\n",
            "Epoch 30: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2124 - accuracy: 0.9820 - val_loss: 0.8097 - val_accuracy: 0.8867\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2369 - accuracy: 0.9820\n",
            "Epoch 31: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2369 - accuracy: 0.9820 - val_loss: 1.8140 - val_accuracy: 0.7333\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.9540\n",
            "Epoch 32: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3359 - accuracy: 0.9540 - val_loss: 1.1905 - val_accuracy: 0.7733\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9780\n",
            "Epoch 33: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2233 - accuracy: 0.9780 - val_loss: 0.9714 - val_accuracy: 0.8467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9880\n",
            "Epoch 34: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1850 - accuracy: 0.9880 - val_loss: 0.7114 - val_accuracy: 0.8733\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9880\n",
            "Epoch 35: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2054 - accuracy: 0.9880 - val_loss: 0.7519 - val_accuracy: 0.8533\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9880\n",
            "Epoch 36: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1915 - accuracy: 0.9880 - val_loss: 0.6453 - val_accuracy: 0.8600\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9940\n",
            "Epoch 37: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1905 - accuracy: 0.9940 - val_loss: 0.6395 - val_accuracy: 0.8733\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9900\n",
            "Epoch 38: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1808 - accuracy: 0.9900 - val_loss: 0.7570 - val_accuracy: 0.8600\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9940\n",
            "Epoch 39: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1725 - accuracy: 0.9940 - val_loss: 0.6548 - val_accuracy: 0.8867\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9980\n",
            "Epoch 40: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.1657 - accuracy: 0.9980 - val_loss: 0.7762 - val_accuracy: 0.8733\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0363 - accuracy: 0.8020\n",
            "Epoch 1: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 1.0363 - accuracy: 0.8020 - val_loss: 2.5945 - val_accuracy: 0.5600\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.8420\n",
            "Epoch 2: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.6840 - accuracy: 0.8420 - val_loss: 1.1267 - val_accuracy: 0.7400\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5970 - accuracy: 0.8580\n",
            "Epoch 3: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.5970 - accuracy: 0.8580 - val_loss: 0.8630 - val_accuracy: 0.7800\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.9140\n",
            "Epoch 4: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.4094 - accuracy: 0.9140 - val_loss: 0.6997 - val_accuracy: 0.8400\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.9460\n",
            "Epoch 5: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3581 - accuracy: 0.9460 - val_loss: 0.6226 - val_accuracy: 0.8667\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3396 - accuracy: 0.9420\n",
            "Epoch 6: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3396 - accuracy: 0.9420 - val_loss: 0.8867 - val_accuracy: 0.8200\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.9720\n",
            "Epoch 7: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2737 - accuracy: 0.9720 - val_loss: 0.7532 - val_accuracy: 0.8467\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.9600\n",
            "Epoch 8: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.3054 - accuracy: 0.9600 - val_loss: 0.8069 - val_accuracy: 0.8400\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2529 - accuracy: 0.9660\n",
            "Epoch 9: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2529 - accuracy: 0.9660 - val_loss: 0.6659 - val_accuracy: 0.8533\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2125 - accuracy: 0.9840\n",
            "Epoch 10: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2125 - accuracy: 0.9840 - val_loss: 0.6968 - val_accuracy: 0.8667\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2063 - accuracy: 0.9840\n",
            "Epoch 11: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2063 - accuracy: 0.9840 - val_loss: 0.6665 - val_accuracy: 0.8667\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9840\n",
            "Epoch 12: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2175 - accuracy: 0.9840 - val_loss: 0.6891 - val_accuracy: 0.8467\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9780\n",
            "Epoch 13: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2058 - accuracy: 0.9780 - val_loss: 0.6348 - val_accuracy: 0.8733\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2052 - accuracy: 0.9860\n",
            "Epoch 14: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2052 - accuracy: 0.9860 - val_loss: 0.6398 - val_accuracy: 0.8533\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1950 - accuracy: 0.9880\n",
            "Epoch 15: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1950 - accuracy: 0.9880 - val_loss: 0.8367 - val_accuracy: 0.8200\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2151 - accuracy: 0.9800\n",
            "Epoch 16: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2151 - accuracy: 0.9800 - val_loss: 0.8066 - val_accuracy: 0.8267\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9880\n",
            "Epoch 17: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.1823 - accuracy: 0.9880 - val_loss: 0.6829 - val_accuracy: 0.8400\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9900\n",
            "Epoch 18: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 284ms/step - loss: 0.1850 - accuracy: 0.9900 - val_loss: 0.6617 - val_accuracy: 0.8333\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1751 - accuracy: 0.9920\n",
            "Epoch 19: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1751 - accuracy: 0.9920 - val_loss: 0.7376 - val_accuracy: 0.8800\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9940\n",
            "Epoch 20: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1672 - accuracy: 0.9940 - val_loss: 0.8077 - val_accuracy: 0.8467\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9900\n",
            "Epoch 21: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1736 - accuracy: 0.9900 - val_loss: 0.8046 - val_accuracy: 0.8267\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9860\n",
            "Epoch 22: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1900 - accuracy: 0.9860 - val_loss: 0.9987 - val_accuracy: 0.8533\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2414 - accuracy: 0.9700\n",
            "Epoch 23: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2414 - accuracy: 0.9700 - val_loss: 4.0639 - val_accuracy: 0.5667\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.9500\n",
            "Epoch 24: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3098 - accuracy: 0.9500 - val_loss: 2.1443 - val_accuracy: 0.7067\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.9700\n",
            "Epoch 25: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3080 - accuracy: 0.9700 - val_loss: 1.4049 - val_accuracy: 0.7267\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9580\n",
            "Epoch 26: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 291ms/step - loss: 0.2842 - accuracy: 0.9580 - val_loss: 0.9635 - val_accuracy: 0.8067\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9600\n",
            "Epoch 27: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2609 - accuracy: 0.9600 - val_loss: 1.0817 - val_accuracy: 0.8133\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9740\n",
            "Epoch 28: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2288 - accuracy: 0.9740 - val_loss: 1.2919 - val_accuracy: 0.7867\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9940\n",
            "Epoch 29: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1785 - accuracy: 0.9940 - val_loss: 1.3266 - val_accuracy: 0.8267\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2513 - accuracy: 0.9740\n",
            "Epoch 30: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2513 - accuracy: 0.9740 - val_loss: 1.9420 - val_accuracy: 0.6867\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9640\n",
            "Epoch 31: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2779 - accuracy: 0.9640 - val_loss: 1.7534 - val_accuracy: 0.7733\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9760\n",
            "Epoch 32: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2384 - accuracy: 0.9760 - val_loss: 1.4168 - val_accuracy: 0.7533\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.9740\n",
            "Epoch 33: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2494 - accuracy: 0.9740 - val_loss: 1.0810 - val_accuracy: 0.8000\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9880\n",
            "Epoch 34: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1930 - accuracy: 0.9880 - val_loss: 1.0839 - val_accuracy: 0.8133\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9940\n",
            "Epoch 35: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1920 - accuracy: 0.9940 - val_loss: 1.0971 - val_accuracy: 0.7800\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9800\n",
            "Epoch 36: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2224 - accuracy: 0.9800 - val_loss: 1.2546 - val_accuracy: 0.7800\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9880\n",
            "Epoch 37: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2159 - accuracy: 0.9880 - val_loss: 1.0556 - val_accuracy: 0.8000\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9880\n",
            "Epoch 38: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2120 - accuracy: 0.9880 - val_loss: 1.0963 - val_accuracy: 0.7867\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1998 - accuracy: 0.9900\n",
            "Epoch 39: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1998 - accuracy: 0.9900 - val_loss: 1.7024 - val_accuracy: 0.7067\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.9820\n",
            "Epoch 40: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2039 - accuracy: 0.9820 - val_loss: 1.6804 - val_accuracy: 0.7533\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.0836 - accuracy: 0.8000\n",
            "Epoch 1: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 1.0836 - accuracy: 0.8000 - val_loss: 1.4082 - val_accuracy: 0.6733\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.7679 - accuracy: 0.8080\n",
            "Epoch 2: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.7679 - accuracy: 0.8080 - val_loss: 1.1611 - val_accuracy: 0.7067\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.8720\n",
            "Epoch 3: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.5516 - accuracy: 0.8720 - val_loss: 0.8779 - val_accuracy: 0.7933\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.9320\n",
            "Epoch 4: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.4080 - accuracy: 0.9320 - val_loss: 0.8877 - val_accuracy: 0.8133\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9540\n",
            "Epoch 5: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3303 - accuracy: 0.9540 - val_loss: 0.7930 - val_accuracy: 0.8667\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.9440\n",
            "Epoch 6: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3129 - accuracy: 0.9440 - val_loss: 0.7125 - val_accuracy: 0.8667\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.9660\n",
            "Epoch 7: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2830 - accuracy: 0.9660 - val_loss: 0.7705 - val_accuracy: 0.8800\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.9600\n",
            "Epoch 8: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3182 - accuracy: 0.9600 - val_loss: 0.5164 - val_accuracy: 0.8933\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.9480\n",
            "Epoch 9: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3407 - accuracy: 0.9480 - val_loss: 0.6969 - val_accuracy: 0.8733\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3145 - accuracy: 0.9460\n",
            "Epoch 10: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3145 - accuracy: 0.9460 - val_loss: 0.9021 - val_accuracy: 0.8200\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2521 - accuracy: 0.9700\n",
            "Epoch 11: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2521 - accuracy: 0.9700 - val_loss: 0.7887 - val_accuracy: 0.8333\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3112 - accuracy: 0.9700\n",
            "Epoch 12: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3112 - accuracy: 0.9700 - val_loss: 0.7222 - val_accuracy: 0.8533\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2813 - accuracy: 0.9680\n",
            "Epoch 13: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2813 - accuracy: 0.9680 - val_loss: 0.6824 - val_accuracy: 0.8467\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2633 - accuracy: 0.9640\n",
            "Epoch 14: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2633 - accuracy: 0.9640 - val_loss: 0.6296 - val_accuracy: 0.8800\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9800\n",
            "Epoch 15: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2313 - accuracy: 0.9800 - val_loss: 0.5281 - val_accuracy: 0.9000\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.9660\n",
            "Epoch 16: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2752 - accuracy: 0.9660 - val_loss: 0.7350 - val_accuracy: 0.8733\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9680\n",
            "Epoch 17: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2570 - accuracy: 0.9680 - val_loss: 0.7532 - val_accuracy: 0.8467\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9800\n",
            "Epoch 18: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2271 - accuracy: 0.9800 - val_loss: 0.7168 - val_accuracy: 0.8867\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9840\n",
            "Epoch 19: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1979 - accuracy: 0.9840 - val_loss: 0.6968 - val_accuracy: 0.8733\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9800\n",
            "Epoch 20: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2149 - accuracy: 0.9800 - val_loss: 0.6005 - val_accuracy: 0.8800\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.9820\n",
            "Epoch 21: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2129 - accuracy: 0.9820 - val_loss: 0.6890 - val_accuracy: 0.8600\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2143 - accuracy: 0.9820\n",
            "Epoch 22: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2143 - accuracy: 0.9820 - val_loss: 0.9320 - val_accuracy: 0.8333\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9820\n",
            "Epoch 23: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2091 - accuracy: 0.9820 - val_loss: 0.9700 - val_accuracy: 0.8400\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9880\n",
            "Epoch 24: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 298ms/step - loss: 0.2117 - accuracy: 0.9880 - val_loss: 0.6919 - val_accuracy: 0.8867\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9820\n",
            "Epoch 25: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2012 - accuracy: 0.9820 - val_loss: 0.7918 - val_accuracy: 0.8533\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9900\n",
            "Epoch 26: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1790 - accuracy: 0.9900 - val_loss: 0.7219 - val_accuracy: 0.8400\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1767 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1767 - accuracy: 0.9920 - val_loss: 0.6681 - val_accuracy: 0.8867\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.9940\n",
            "Epoch 28: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1785 - accuracy: 0.9940 - val_loss: 0.6441 - val_accuracy: 0.8933\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9820\n",
            "Epoch 29: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2283 - accuracy: 0.9820 - val_loss: 0.8664 - val_accuracy: 0.8600\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9760\n",
            "Epoch 30: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2194 - accuracy: 0.9760 - val_loss: 0.6237 - val_accuracy: 0.8867\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9900\n",
            "Epoch 31: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1850 - accuracy: 0.9900 - val_loss: 0.6135 - val_accuracy: 0.8800\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9920\n",
            "Epoch 32: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1755 - accuracy: 0.9920 - val_loss: 0.6026 - val_accuracy: 0.8867\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9860\n",
            "Epoch 33: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1973 - accuracy: 0.9860 - val_loss: 1.0206 - val_accuracy: 0.8467\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3994 - accuracy: 0.9460\n",
            "Epoch 34: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.3994 - accuracy: 0.9460 - val_loss: 1.2844 - val_accuracy: 0.7933\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4026 - accuracy: 0.9300\n",
            "Epoch 35: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4026 - accuracy: 0.9300 - val_loss: 1.1694 - val_accuracy: 0.7867\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.9600\n",
            "Epoch 36: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2774 - accuracy: 0.9600 - val_loss: 1.0631 - val_accuracy: 0.8267\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2256 - accuracy: 0.9740\n",
            "Epoch 37: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2256 - accuracy: 0.9740 - val_loss: 0.8443 - val_accuracy: 0.8667\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.9920\n",
            "Epoch 38: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 301ms/step - loss: 0.1882 - accuracy: 0.9920 - val_loss: 0.8417 - val_accuracy: 0.8467\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1854 - accuracy: 0.9920\n",
            "Epoch 39: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1854 - accuracy: 0.9920 - val_loss: 0.9615 - val_accuracy: 0.8333\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9980\n",
            "Epoch 40: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1687 - accuracy: 0.9980 - val_loss: 0.9855 - val_accuracy: 0.8667\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9023 - accuracy: 0.8380\n",
            "Epoch 1: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 296ms/step - loss: 0.9023 - accuracy: 0.8380 - val_loss: 1.1427 - val_accuracy: 0.7800\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.8800\n",
            "Epoch 2: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.6134 - accuracy: 0.8800 - val_loss: 0.7996 - val_accuracy: 0.8333\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.8880\n",
            "Epoch 3: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.4777 - accuracy: 0.8880 - val_loss: 0.6733 - val_accuracy: 0.8467\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3733 - accuracy: 0.9380\n",
            "Epoch 4: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3733 - accuracy: 0.9380 - val_loss: 0.6413 - val_accuracy: 0.8333\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.9640\n",
            "Epoch 5: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2853 - accuracy: 0.9640 - val_loss: 0.6043 - val_accuracy: 0.8800\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.9720\n",
            "Epoch 6: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2539 - accuracy: 0.9720 - val_loss: 0.7428 - val_accuracy: 0.8267\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9720\n",
            "Epoch 7: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.2550 - accuracy: 0.9720 - val_loss: 1.6577 - val_accuracy: 0.7200\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2562 - accuracy: 0.9740\n",
            "Epoch 8: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2562 - accuracy: 0.9740 - val_loss: 0.9415 - val_accuracy: 0.8200\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2962 - accuracy: 0.9560\n",
            "Epoch 9: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2962 - accuracy: 0.9560 - val_loss: 0.7428 - val_accuracy: 0.8600\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9780\n",
            "Epoch 10: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2343 - accuracy: 0.9780 - val_loss: 0.6861 - val_accuracy: 0.8733\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.9820\n",
            "Epoch 11: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2004 - accuracy: 0.9820 - val_loss: 0.6724 - val_accuracy: 0.8600\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1979 - accuracy: 0.9860\n",
            "Epoch 12: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1979 - accuracy: 0.9860 - val_loss: 0.5751 - val_accuracy: 0.8667\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9860\n",
            "Epoch 13: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2011 - accuracy: 0.9860 - val_loss: 0.7582 - val_accuracy: 0.8733\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9700\n",
            "Epoch 14: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2426 - accuracy: 0.9700 - val_loss: 0.9389 - val_accuracy: 0.8133\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9820\n",
            "Epoch 15: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2090 - accuracy: 0.9820 - val_loss: 0.8584 - val_accuracy: 0.8200\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9820\n",
            "Epoch 16: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2117 - accuracy: 0.9820 - val_loss: 0.7130 - val_accuracy: 0.8600\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9880\n",
            "Epoch 17: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1940 - accuracy: 0.9880 - val_loss: 0.6943 - val_accuracy: 0.8467\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9560\n",
            "Epoch 18: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2717 - accuracy: 0.9560 - val_loss: 0.9355 - val_accuracy: 0.8200\n",
            "Epoch 19/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9640\n",
            "Epoch 19: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2683 - accuracy: 0.9640 - val_loss: 0.9569 - val_accuracy: 0.8200\n",
            "Epoch 20/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9720\n",
            "Epoch 20: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2793 - accuracy: 0.9720 - val_loss: 1.1418 - val_accuracy: 0.7867\n",
            "Epoch 21/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.9560\n",
            "Epoch 21: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2671 - accuracy: 0.9560 - val_loss: 0.7596 - val_accuracy: 0.8667\n",
            "Epoch 22/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 0.9860\n",
            "Epoch 22: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2104 - accuracy: 0.9860 - val_loss: 1.0467 - val_accuracy: 0.8400\n",
            "Epoch 23/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3211 - accuracy: 0.9600\n",
            "Epoch 23: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.3211 - accuracy: 0.9600 - val_loss: 1.3084 - val_accuracy: 0.7533\n",
            "Epoch 24/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2337 - accuracy: 0.9760\n",
            "Epoch 24: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2337 - accuracy: 0.9760 - val_loss: 1.1548 - val_accuracy: 0.8067\n",
            "Epoch 25/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9860\n",
            "Epoch 25: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2024 - accuracy: 0.9860 - val_loss: 0.9448 - val_accuracy: 0.8533\n",
            "Epoch 26/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9940\n",
            "Epoch 26: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1724 - accuracy: 0.9940 - val_loss: 0.7373 - val_accuracy: 0.8733\n",
            "Epoch 27/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.9920\n",
            "Epoch 27: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.1776 - accuracy: 0.9920 - val_loss: 0.6803 - val_accuracy: 0.8867\n",
            "Epoch 28/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9880\n",
            "Epoch 28: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1904 - accuracy: 0.9880 - val_loss: 0.8972 - val_accuracy: 0.8400\n",
            "Epoch 29/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9960\n",
            "Epoch 29: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.1694 - accuracy: 0.9960 - val_loss: 0.8313 - val_accuracy: 0.8600\n",
            "Epoch 30/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1681 - accuracy: 0.9940\n",
            "Epoch 30: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1681 - accuracy: 0.9940 - val_loss: 0.6736 - val_accuracy: 0.8867\n",
            "Epoch 31/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 1.0000\n",
            "Epoch 31: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1571 - accuracy: 1.0000 - val_loss: 0.6287 - val_accuracy: 0.9000\n",
            "Epoch 32/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9940\n",
            "Epoch 32: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 289ms/step - loss: 0.1628 - accuracy: 0.9940 - val_loss: 0.7383 - val_accuracy: 0.8800\n",
            "Epoch 33/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1982 - accuracy: 0.9880\n",
            "Epoch 33: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.1982 - accuracy: 0.9880 - val_loss: 1.3199 - val_accuracy: 0.7933\n",
            "Epoch 34/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9720\n",
            "Epoch 34: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2406 - accuracy: 0.9720 - val_loss: 0.5834 - val_accuracy: 0.8867\n",
            "Epoch 35/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9620\n",
            "Epoch 35: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2622 - accuracy: 0.9620 - val_loss: 0.6975 - val_accuracy: 0.8667\n",
            "Epoch 36/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.9580\n",
            "Epoch 36: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.3559 - accuracy: 0.9580 - val_loss: 1.2579 - val_accuracy: 0.8000\n",
            "Epoch 37/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.9560\n",
            "Epoch 37: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2810 - accuracy: 0.9560 - val_loss: 0.7911 - val_accuracy: 0.8467\n",
            "Epoch 38/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9860\n",
            "Epoch 38: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1980 - accuracy: 0.9860 - val_loss: 2.0011 - val_accuracy: 0.7133\n",
            "Epoch 39/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9800\n",
            "Epoch 39: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 292ms/step - loss: 0.2158 - accuracy: 0.9800 - val_loss: 0.8761 - val_accuracy: 0.8267\n",
            "Epoch 40/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1929 - accuracy: 0.9920\n",
            "Epoch 40: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1929 - accuracy: 0.9920 - val_loss: 0.6199 - val_accuracy: 0.8933\n",
            "(500, 224, 224, 3) (500,)\n",
            "Epoch 1/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.9058 - accuracy: 0.8240\n",
            "Epoch 1: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.9058 - accuracy: 0.8240 - val_loss: 1.0925 - val_accuracy: 0.8000\n",
            "Epoch 2/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.6651 - accuracy: 0.8320\n",
            "Epoch 2: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 283ms/step - loss: 0.6651 - accuracy: 0.8320 - val_loss: 0.6579 - val_accuracy: 0.8467\n",
            "Epoch 3/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.9120\n",
            "Epoch 3: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.4412 - accuracy: 0.9120 - val_loss: 0.6742 - val_accuracy: 0.7933\n",
            "Epoch 4/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.9460\n",
            "Epoch 4: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3654 - accuracy: 0.9460 - val_loss: 0.9231 - val_accuracy: 0.7933\n",
            "Epoch 5/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.9360\n",
            "Epoch 5: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.3603 - accuracy: 0.9360 - val_loss: 0.6141 - val_accuracy: 0.8467\n",
            "Epoch 6/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.9560\n",
            "Epoch 6: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2919 - accuracy: 0.9560 - val_loss: 0.5837 - val_accuracy: 0.8800\n",
            "Epoch 7/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9560\n",
            "Epoch 7: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 285ms/step - loss: 0.2878 - accuracy: 0.9560 - val_loss: 0.6446 - val_accuracy: 0.8533\n",
            "Epoch 8/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2660 - accuracy: 0.9620\n",
            "Epoch 8: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2660 - accuracy: 0.9620 - val_loss: 0.5436 - val_accuracy: 0.8867\n",
            "Epoch 9/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.9700\n",
            "Epoch 9: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2710 - accuracy: 0.9700 - val_loss: 0.7647 - val_accuracy: 0.8400\n",
            "Epoch 10/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9640\n",
            "Epoch 10: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2419 - accuracy: 0.9640 - val_loss: 0.7342 - val_accuracy: 0.8733\n",
            "Epoch 11/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9780\n",
            "Epoch 11: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2116 - accuracy: 0.9780 - val_loss: 0.6675 - val_accuracy: 0.8667\n",
            "Epoch 12/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9800\n",
            "Epoch 12: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 290ms/step - loss: 0.2188 - accuracy: 0.9800 - val_loss: 0.5370 - val_accuracy: 0.8867\n",
            "Epoch 13/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9780\n",
            "Epoch 13: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.2266 - accuracy: 0.9780 - val_loss: 0.7732 - val_accuracy: 0.8800\n",
            "Epoch 14/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9860\n",
            "Epoch 14: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.1930 - accuracy: 0.9860 - val_loss: 0.6110 - val_accuracy: 0.8933\n",
            "Epoch 15/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9840\n",
            "Epoch 15: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2387 - accuracy: 0.9840 - val_loss: 0.4702 - val_accuracy: 0.9000\n",
            "Epoch 16/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.9440\n",
            "Epoch 16: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 286ms/step - loss: 0.3696 - accuracy: 0.9440 - val_loss: 1.2263 - val_accuracy: 0.7333\n",
            "Epoch 17/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2483 - accuracy: 0.9700\n",
            "Epoch 17: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 288ms/step - loss: 0.2483 - accuracy: 0.9700 - val_loss: 1.4184 - val_accuracy: 0.7333\n",
            "Epoch 18/40\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.9700\n",
            "Epoch 18: val_accuracy did not improve from 0.92667\n",
            "16/16 [==============================] - 5s 287ms/step - loss: 0.2496 - accuracy: 0.9700 - val_loss: 1.1353 - val_accuracy: 0.7733\n",
            "Epoch 19/40\n",
            " 7/16 [============>.................] - ETA: 2s - loss: 0.2768 - accuracy: 0.9732"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "eeb7532df2be4cf2ba25691eabeb48d4",
            "fb00df8ffc1a4eae933e70287ed9bf48",
            "582a0b0515dc4e8096abffaf51506f6a",
            "fd5e7d44dd5d4782a6f9f5ee066ec923",
            "81b4998273d5484ea8362089c2f0ae70",
            "627297e51f824f90911882ff54500c89",
            "3167c62a5cac4fcab95e81d1fe5a8a6d",
            "9ae0b54c54294174b8428a2fd1f25a90"
          ]
        },
        "outputId": "6e16d3c2-16ae-467e-8f0a-3d3db85e3a21",
        "id": "TkzZjyVIf6eN"
      },
      "source": [
        "# evaluating model on 10% of test dataset\n",
        "pred_1 = model.test_on_dataset(d_test, limit=0.1)\n",
        "Metrics.print_all(d_test.labels[:len(pred_1)], pred_1, '100% of test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeb7532df2be4cf2ba25691eabeb48d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "metrics for 100.0% of test:\n",
            "['0 : 0', '0 : 0', '0 : 0', '0 : 0', '0 : 0']\n",
            "\t accuracy 0.8836:\n",
            "\t balanced accuracy 0.8836:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwvHVVzVWZ5"
      },
      "source": [
        "Пример тестирования модели на полном наборе данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjI_sbMi3TMY"
      },
      "source": [
        "# evaluating model on full test dataset (may take time)\n",
        "if TEST_ON_LARGE_DATASET:\n",
        "    pred_2 = Model.stacked_test(d_test, ['models/best6', 'models/best_resnet', 'models/best_densenet'])\n",
        "    Metrics.print_all(d_test.labels, pred_2, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvyEHdxEB18o"
      },
      "source": [
        "Результат работы пайплайна обучения и тестирования выше тоже будет оцениваться. Поэтому не забудьте присылать на проверку ноутбук с выполнеными ячейками кода с демонстрациями метрик обучения, графиками и т.п. В этом пайплайне Вам необходимо продемонстрировать работу всех реализованных дополнений, улучшений и т.п.\n",
        "\n",
        "<font color=\"red\">\n",
        "Настоятельно рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и прислать этот pdf вместе с самим ноутбуком.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzSKAvVI6uCW"
      },
      "source": [
        "### Тестирование модели на других наборах данных\n",
        "\n",
        "Ваша модель должна поддерживать тестирование на других наборах данных. Для удобства, Вам предоставляется набор данных test_tiny, который представляет собой малую часть (2% изображений) набора test. Ниже приведен фрагмент кода, который будет осуществлять тестирование для оценивания Вашей модели на дополнительных тестовых наборах данных.\n",
        "\n",
        "<font color=\"red\">\n",
        "Прежде чем отсылать задание на проверку, убедитесь в работоспособности фрагмента кода ниже.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdY3uTt87tqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351,
          "referenced_widgets": [
            "b89ce09305dd41a18deb794ce9e7abbd",
            "ba564b750d86485e82901e28137d4fa0",
            "ebaf8c08b4104513809b5dfefd3f6517",
            "7ef64ea579804abf9976e0a2100e14a3",
            "83ccdf2320154bf2a51aba0ea5be1940",
            "30788e32776a478e9690bc34e6b12280",
            "4cc07e25ecac4f9fbfa825d35e1b5bb0",
            "769399236a864646b3b8b9b278622eae",
            "ffa971f148bc42ecb4e173f94b8a1739",
            "023b5d9fd72846af839cf2bcc5c9847d",
            "4edc65a77c5741728dcbb9c9589fb03a",
            "436d9940170d42fcb3ff16db58e1e9f4",
            "d6c23b5176ac41a48d3131eb5c536e08",
            "f048431f8081478094558c41b101ed7f",
            "487a811daf9544bea5886f00c784c5ad",
            "89e4753bd37c430f87394deb0942bcd5",
            "31052ab1ca834bda8b95b72ea6c17989",
            "5b58de45e885424fb3daa6bd6d697cef",
            "eea53d26cb534f93972921f3e2376591",
            "22dc30bcd04347198d9e4d977bd6cb88",
            "2df74b8b1be849de9421bc291db7ed6c",
            "1b1aece8cda44142910d80b28771469a",
            "a3f6032308a54b0584b6593d288a8b24",
            "5fd15a514e1144418ab6408e0fd9f7b1"
          ]
        },
        "outputId": "39ce3450-f2b9-426a-9a9d-bf92bd8b4036"
      },
      "source": [
        "d_test_tiny = Dataset('test', PROJECT_DIR)\n",
        "pred = Model.stacked_test(d_test_tiny, ['models/best6', 'models/best_resnet', 'models/best_densenet'])\n",
        "Metrics.print_all(d_test_tiny.labels, pred, 'test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset test from npz.\n",
            "Done. Dataset test consists of 4500 images.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b89ce09305dd41a18deb794ce9e7abbd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model models/best6 evaluated\n",
            "Changing preprocessor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffa971f148bc42ecb4e173f94b8a1739",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model models/best_resnet evaluated\n",
            "Changing preprocessor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31052ab1ca834bda8b95b72ea6c17989",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=45.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model models/best_densenet evaluated\n",
            "metrics for test:\n",
            "['0 : 0', '0 : 0', '0 : 0', '0 : 0', '0 : 0']\n",
            "\t accuracy 0.9544:\n",
            "\t balanced accuracy 0.9544:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPvyj4gscU10"
      },
      "source": [
        "Отмонтировать Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfX35zNSvFWn"
      },
      "source": [
        "drive.flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMyDxCDCspcI"
      },
      "source": [
        "---\n",
        "# Дополнительные \"полезности\"\n",
        "\n",
        "Ниже приведены примеры использования различных функций и библиотек, которые могут быть полезны при выполнении данного практического задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvLwSttCs1rB"
      },
      "source": [
        "### Измерение времени работы кода\n",
        "\n",
        "Измерять время работы какой-либо функции можно легко и непринужденно при помощи функции timeit из соответствующего модуля:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HnLVhwE9C9S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fa95fae8-7553-4c53-ffdb-eab00b19e1b8"
      },
      "source": [
        "'''import timeit\n",
        "\n",
        "def factorial(n):\n",
        "    res = 1\n",
        "    for i in range(1, n + 1):\n",
        "        res *= i\n",
        "    return res\n",
        "\n",
        "\n",
        "def f():\n",
        "    return factorial(n=1000)\n",
        "\n",
        "n_runs = 128\n",
        "print(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"import timeit\\n\\ndef factorial(n):\\n    res = 1\\n    for i in range(1, n + 1):\\n        res *= i\\n    return res\\n\\n\\ndef f():\\n    return factorial(n=1000)\\n\\nn_runs = 128\\nprint(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibGVEdguOOi"
      },
      "source": [
        "### Scikit-learn\n",
        "\n",
        "Для использования \"классических\" алгоритмов машинного обучения рекомендуется использовать библиотеку scikit-learn (https://scikit-learn.org/stable/). Пример классификации изображений цифр из набора данных MNIST при помощи классификатора SVM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHnBzEfunAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "3aac7130-7c41-4243-fc35-737401741c0b"
      },
      "source": [
        "'''# Standard scientific Python imports\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import datasets, classifiers and performance metrics\n",
        "from sklearn import datasets, svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# The data that we are interested in is made of 8x8 images of digits, let's\n",
        "# have a look at the first 4 images, stored in the `images` attribute of the\n",
        "# dataset.  If we were working from image files, we could load them using\n",
        "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
        "# images, we know which digit they represent: it is given in the 'target' of\n",
        "# the dataset.\n",
        "_, axes = plt.subplots(2, 4)\n",
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)\n",
        "\n",
        "# To apply a classifier on this data, we need to flatten the image, to\n",
        "# turn the data in a (samples, feature) matrix:\n",
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "\n",
        "# Create a classifier: a support vector classifier\n",
        "classifier = svm.SVC(gamma=0.001)\n",
        "\n",
        "# Split data into train and test subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, digits.target, test_size=0.5, shuffle=False)\n",
        "\n",
        "# We learn the digits on the first half of the digits\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Now predict the value of the digit on the second half:\n",
        "predicted = classifier.predict(X_test)\n",
        "\n",
        "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
        "for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Prediction: %i' % prediction)\n",
        "\n",
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
        "disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
        "\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Standard scientific Python imports\\nimport matplotlib.pyplot as plt\\n\\n# Import datasets, classifiers and performance metrics\\nfrom sklearn import datasets, svm, metrics\\nfrom sklearn.model_selection import train_test_split\\n\\n# The digits dataset\\ndigits = datasets.load_digits()\\n\\n# The data that we are interested in is made of 8x8 images of digits, let\\'s\\n# have a look at the first 4 images, stored in the `images` attribute of the\\n# dataset.  If we were working from image files, we could load them using\\n# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\\n# images, we know which digit they represent: it is given in the \\'target\\' of\\n# the dataset.\\n_, axes = plt.subplots(2, 4)\\nimages_and_labels = list(zip(digits.images, digits.target))\\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\\n    ax.set_axis_off()\\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\\'nearest\\')\\n    ax.set_title(\\'Training: %i\\' % label)\\n\\n# To apply a classifier on this data, we need to flatten the image, to\\n# turn the data in a (samples, feature) matrix:\\nn_samples = len(digits.images)\\ndata = digits.images.reshape((n_samples, -1))\\n\\n# Create a classifier: a support vector classifier\\nclassifier = svm.SVC(gamma=0.001)\\n\\n# Split data into train and test subsets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    data, digits.target, test_size=0.5, shuffle=False)\\n\\n# We learn the digits on the first half of the digits\\nclassifier.fit(X_train, y_train)\\n\\n# Now predict the value of the digit on the second half:\\npredicted = classifier.predict(X_test)\\n\\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\\nfor ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\\n    ax.set_axis_off()\\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\\'nearest\\')\\n    ax.set_title(\\'Prediction: %i\\' % prediction)\\n\\nprint(\"Classification report for classifier %s:\\n%s\\n\"\\n      % (classifier, metrics.classification_report(y_test, predicted)))\\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\\ndisp.figure_.suptitle(\"Confusion Matrix\")\\nprint(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\\n\\nplt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu3Dny5zxcVy"
      },
      "source": [
        "### Scikit-image\n",
        "\n",
        "Реализовывать различные операции для работы с изображениями можно как самостоятельно, работая с массивами numpy, так и используя специализированные библиотеки, например, scikit-image (https://scikit-image.org/). Ниже приведен пример использования Canny edge detector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TZvy_d7xc0B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "c1f175fd-8add-428e-b119-2c8aaebf9969"
      },
      "source": [
        "'''import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "from skimage import feature\n",
        "\n",
        "\n",
        "# Generate noisy image of a square\n",
        "im = np.zeros((128, 128))\n",
        "im[32:-32, 32:-32] = 1\n",
        "\n",
        "im = ndi.rotate(im, 15, mode='constant')\n",
        "im = ndi.gaussian_filter(im, 4)\n",
        "im += 0.2 * np.random.random(im.shape)\n",
        "\n",
        "# Compute the Canny filter for two values of sigma\n",
        "edges1 = feature.canny(im)\n",
        "edges2 = feature.canny(im, sigma=3)\n",
        "\n",
        "# display results\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\n",
        "                                    sharex=True, sharey=True)\n",
        "\n",
        "ax1.imshow(im, cmap=plt.cm.gray)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('noisy image', fontsize=20)\n",
        "\n",
        "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
        "ax2.axis('off')\n",
        "ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\n",
        "\n",
        "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
        "ax3.axis('off')\n",
        "ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"import numpy as np\\nimport matplotlib.pyplot as plt\\nfrom scipy import ndimage as ndi\\n\\nfrom skimage import feature\\n\\n\\n# Generate noisy image of a square\\nim = np.zeros((128, 128))\\nim[32:-32, 32:-32] = 1\\n\\nim = ndi.rotate(im, 15, mode='constant')\\nim = ndi.gaussian_filter(im, 4)\\nim += 0.2 * np.random.random(im.shape)\\n\\n# Compute the Canny filter for two values of sigma\\nedges1 = feature.canny(im)\\nedges2 = feature.canny(im, sigma=3)\\n\\n# display results\\nfig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\\n                                    sharex=True, sharey=True)\\n\\nax1.imshow(im, cmap=plt.cm.gray)\\nax1.axis('off')\\nax1.set_title('noisy image', fontsize=20)\\n\\nax2.imshow(edges1, cmap=plt.cm.gray)\\nax2.axis('off')\\nax2.set_title(r'Canny filter, $\\\\sigma=1$', fontsize=20)\\n\\nax3.imshow(edges2, cmap=plt.cm.gray)\\nax3.axis('off')\\nax3.set_title(r'Canny filter, $\\\\sigma=3$', fontsize=20)\\n\\nfig.tight_layout()\\n\\nplt.show()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiEWhGUQRGoH"
      },
      "source": [
        "### Tensorflow 2\n",
        "\n",
        "Для создания и обучения нейросетевых моделей можно использовать фреймворк глубокого обучения Tensorflow 2. Ниже приведен пример простейшей нейроной сети, использующейся для классификации изображений из набора данных MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDwLG7A1ReNy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "11751b88-c82d-406f-f937-1917eb7d0c87"
      },
      "source": [
        "'''# Install TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "model.evaluate(x_test,  y_test, verbose=2)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# Install TensorFlow\\n\\nimport tensorflow as tf\\n\\nmnist = tf.keras.datasets.mnist\\n\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\nx_train, x_test = x_train / 255.0, x_test / 255.0\\n\\nmodel = tf.keras.models.Sequential([\\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\\n  tf.keras.layers.Dense(128, activation='relu'),\\n  tf.keras.layers.Dropout(0.2),\\n  tf.keras.layers.Dense(10, activation='softmax')\\n])\\n\\nmodel.compile(optimizer='adam',\\n              loss='sparse_categorical_crossentropy',\\n              metrics=['accuracy'])\\n\\nmodel.fit(x_train, y_train, epochs=5)\\n\\nmodel.evaluate(x_test,  y_test, verbose=2)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvktmLwRu8g"
      },
      "source": [
        "<font color=\"red\">\n",
        "Для эффективной работы с моделями глубокого обучения убедитесь в том, что в текущей среде Google Colab используется аппаратный ускоритель GPU или TPU. Для смены среды выберите \"среда выполнения\" -> \"сменить среду выполнения\".\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJVNOOU9Sjyf"
      },
      "source": [
        "Большое количество туториалов и примеров с кодом на Tensorflow 2 можно найти на официальном сайте https://www.tensorflow.org/tutorials?hl=ru. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVPs3pYpS0U1"
      },
      "source": [
        "Также, Вам может понадобиться написать собственный генератор данных для Tensorflow 2. Скорее всего он будет достаточно простым, и его легко можно будет реализовать, используя официальную документацию TensorFlow 2. Но, на всякий случай (если не удлось сразу разобраться или хочется вникнуть в тему более глубоко), можете посмотреть следующий отличный туториал: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwI-T0IXyN84"
      },
      "source": [
        "### Numba\n",
        "\n",
        "В некоторых ситуациях, при ручных реализациях графовых алгоритмов, выполнение многократных вложенных циклов for в python можно существенно ускорить, используя JIT-компилятор Numba (https://numba.pydata.org/).\n",
        "Примеры использования Numba в Google Colab можно найти тут:\n",
        "1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb\n",
        "2. https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb \n",
        "\n",
        "> Пожалуйста, если Вы решили использовать Numba для решения этого практического задания, еще раз подумайте, нужно ли это Вам, и есть ли возможность реализовать требуемую функциональность иным способом. Используйте Numba только при реальной необходимости.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxAJ00A76LcF"
      },
      "source": [
        "### Работа с zip архивами в Google Drive\n",
        "\n",
        "Запаковка и распаковка zip архивов может пригодиться при сохранении и загрузки Вашей модели. Ниже приведен фрагмент кода, иллюстрирующий помещение нескольких файлов в zip архив с последующим чтением файлов из него. Все действия с директориями, файлами и архивами должны осущетвляться с примонтированным Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJiKndOpPu_e"
      },
      "source": [
        "Создадим 2 изображения, поместим их в директорию tmp внутри PROJECT_DIR, запакуем директорию tmp в архив tmp.zip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRwgPtv-6nMP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "04229656-9a30-4885-9c89-317ac921558a"
      },
      "source": [
        "'''arr1 = np.random.rand(100, 100, 3) * 255\n",
        "arr2 = np.random.rand(100, 100, 3) * 255\n",
        "\n",
        "img1 = Image.fromarray(arr1.astype('uint8'))\n",
        "img2 = Image.fromarray(arr2.astype('uint8'))\n",
        "\n",
        "p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
        "\n",
        "if not (Path(p) / 'tmp').exists():\n",
        "    (Path(p) / 'tmp').mkdir()\n",
        "\n",
        "img1.save(str(Path(p) / 'tmp' / 'img1.png'))\n",
        "img2.save(str(Path(p) / 'tmp' / 'img2.png'))\n",
        "\n",
        "%cd $p\n",
        "!zip -r \"tmp.zip\" \"tmp\"'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'arr1 = np.random.rand(100, 100, 3) * 255\\narr2 = np.random.rand(100, 100, 3) * 255\\n\\nimg1 = Image.fromarray(arr1.astype(\\'uint8\\'))\\nimg2 = Image.fromarray(arr2.astype(\\'uint8\\'))\\n\\np = \"/content/drive/MyDrive/\" + PROJECT_DIR\\n\\nif not (Path(p) / \\'tmp\\').exists():\\n    (Path(p) / \\'tmp\\').mkdir()\\n\\nimg1.save(str(Path(p) / \\'tmp\\' / \\'img1.png\\'))\\nimg2.save(str(Path(p) / \\'tmp\\' / \\'img2.png\\'))\\n\\n%cd $p\\n!zip -r \"tmp.zip\" \"tmp\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykrBSWNQQlq"
      },
      "source": [
        "Распакуем архив tmp.zip в директорию tmp2 в PROJECT_DIR. Теперь внутри директории tmp2 содержится директория tmp, внутри которой находятся 2 изображения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwSWrYIWMAus",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcb665c1-2531-4b10-b8d4-06cee712787d"
      },
      "source": [
        "'''p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
        "%cd $p\n",
        "!unzip -uq \"tmp.zip\" -d \"tmp2\"'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'p = \"/content/drive/MyDrive/\" + PROJECT_DIR\\n%cd $p\\n!unzip -uq \"tmp.zip\" -d \"tmp2\"'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}